# P8107 Introduction to Mathematical Statistics

## Quick Navigation

[Week 1](#week-1-introduction-to-probability) | [Week 2](#week-2-some-probability-laws) | [Week 3](#week-3-discrete-random-variables) | [Week 4](#week-4-moments-moment-generating-functions-and-continuous-random-variables) | [Week 5](#week-5-some-continuous-random-variables-and-chebyshevs-theorem) | [Week 6](#week-6-joint-marginal-and-conditional-distributions) | [Week 7](#week-7-means-variances-and-covariances-for-bivariate-distributions) | [Week 8](#week-8-midterm-exam) | [Week 9](#week-9-functions-of-random-variables) | [Week 10](#week-10-sampling-distributions-and-the-central-limit-theorem) | [Week 11](#week-11-estimation) | [Week 12](#week-12-properties-of-point-estimation-and-methods-of-estimation) | [Week 13](#week-13-hypothesis-testing) | [Week 14](#week-14-testing-for-means-and-variances-likelihood-ratio-tests) | [Summary](#summary-and-key-points)

## Table of Contents

### [Week 1: Introduction to Probability](#week-1-introduction-to-probability)

- [1.1 Probabilistic Models and Sample Spaces](#11-probabilistic-models-and-sample-spaces-section-22-23)
- [1.2 Basic Probability and Axioms](#12-basic-probability-and-axioms-section-24)
- [1.3 Basic Probability Results and Rules](#13-basic-probability-results-and-rules)
- [1.4 Counting Techniques](#14-counting-techniques-section-26)
- [1.5 Conditional Probability](#15-conditional-probability-section-27)
- [1.6 Independence](#16-independence)

### [Week 2: Some Probability Laws](#week-2-some-probability-laws)

- [2.1 Multiplicative and Additive Laws](#21-multiplicative-and-additive-laws-section-28)
- [2.2 Law of Total Probability and Bayes' Rule](#22-law-of-total-probability-and-bayes-rule-sections-29-210)
- [2.3 Numerical Events and Random Variables](#23-numerical-events-and-random-variables-section-211)
- [2.4 Random Sampling](#24-random-sampling-section-212)

### [Week 3: Discrete Random Variables](#week-3-discrete-random-variables)

- [3.1 Discrete Random Variables Fundamentals](#31-discrete-random-variables-fundamentals-sections-31-33)
- [3.2 Binomial Distribution](#32-binomial-distribution)
- [3.3 Geometric Distribution](#33-geometric-distribution)
- [3.4 Hypergeometric Distribution](#34-hypergeometric-distribution)
- [3.5 Poisson Distribution](#35-poisson-distribution)

### [Week 4: Moments, Moment Generating Functions, and Continuous Random Variables](#week-4-moments-moment-generating-functions-and-continuous-random-variables)

- [4.1 Moments and Moment Generating Functions](#41-moments-and-moment-generating-functions-section-39)
- [4.2 Continuous Random Variables Fundamentals](#42-continuous-random-variables-fundamentals)
- [4.3 Uniform Distribution](#43-uniform-distribution)
- [4.4 Exponential Distribution](#44-exponential-distribution)
- [4.5 Normal Distribution](#45-normal-distribution)
- [4.6 Important Theorems and Properties](#46-important-theorems-and-properties)

### [Week 5: Some Continuous Random Variables and Chebyshev's Theorem](#week-5-some-continuous-random-variables-and-chebyshevs-theorem)

- [5.1 Gamma Distribution](#51-gamma-distribution-section-46)
- [5.2 Beta Distribution](#52-beta-distribution)
- [5.3 Chi-Square Distribution](#53-chi-square-distribution)
- [5.4 Chebyshev's Theorem](#54-chebyshevs-theorem)
- [5.5 Markov's Inequality](#55-markovs-inequality)
- [5.6 Comparison Summary Table](#56-comparison-summary-table)

### [Week 6: Joint, Marginal, and Conditional Distributions](#week-6-joint-marginal-and-conditional-distributions)

- [6.1 Joint Probability Distributions](#61-joint-probability-distributions-sections-51-52)
- [6.2 Marginal Distributions](#62-marginal-distributions)
- [6.3 Conditional Distributions](#63-conditional-distributions)
- [6.4 Independence of Random Variables](#64-independence-of-random-variables)
- [6.5 Transformations of Bivariate Distributions](#65-transformations-of-bivariate-distributions)

### [Week 7: Means, Variances, and Covariances for Bivariate Distributions](#week-7-means-variances-and-covariances-for-bivariate-distributions)

- [7.1 Expected Values for Functions of Random Variables](#71-expected-values-for-functions-of-random-variables-section-55)
- [7.2 Linearity and Sums of RVs](#72-linearity-and-sums-of-rvs)
- [7.3 Covariance and Correlation](#73-covariance-and-correlation)
- [7.4 Variance of Linear Combinations](#74-variance-of-linear-combinations)
- [7.5 Laws of Total Expectation and Variance](#75-laws-of-total-expectation-and-variance)
- [7.6 Sample Mean Properties](#76-sample-mean-properties)
- [7.7 Bivariate Normal and Conditional Results](#77-bivariate-normal-and-conditional-results)
- [7.8 Applications (Portfolio, Aggregation)](#78-applications-portfolio-aggregation)
- [7.9 Applications and Examples](#79-applications-and-examples)

### [Week 8: Midterm Exam](#week-8-midterm-exam)

- [8.1 Midterm Structure and Focus](#81-midterm-structure-and-focus)
- [8.2 Recommended Formula Sheet Inclusions](#82-recommended-formula-sheet-inclusions)
- [8.3 Strategic Review Checklist](#83-strategic-review-checklist)
- [8.4 Common Pitfalls](#84-common-pitfalls)
- [8.5 Rapid Practice Set (Self-Check)](#85-rapid-practice-set-self-check)
- [8.6 Suggested Final 48-Hour Plan](#86-suggested-final-48-hour-plan)
- [8.7 Mental Models Recap](#87-mental-models-recap)

### [Week 9: Functions of Random Variables](#week-9-functions-of-random-variables)

- [9.1 Motivation and Overview](#91-motivation-and-overview-section-61)
- [9.2 Method of Distribution (One-to-One on Support Pieces)](#92-method-of-distribution-one-to-one-on-support-pieces)
- [9.3 Many-to-One Transformations (Partition Method)](#93-many-to-one-transformations-partition-method)
- [9.4 Change of Variables (Multivariate Jacobian Method)](#94-change-of-variables-multivariate-jacobian-method)
- [9.5 Convolutions (Sum of Independent Variables)](#95-convolutions-sum-of-independent-variables)
- [9.6 Ratios and Products](#96-ratios-and-products)
- [9.7 Order Statistics](#97-order-statistics)
- [9.8 Delta Method Preview](#98-delta-method-preview)
- [9.9 Summary Map](#99-summary-map)

### [Week 10: Sampling Distributions and the Central Limit Theorem](#week-10-sampling-distributions-and-the-central-limit-theorem)

- [10.1 Law of Large Numbers (LLN)](#101-law-of-large-numbers-lln)
- [10.2 Central Limit Theorem (CLT)](#102-central-limit-theorem-clt)
- [10.3 Standardizing and Normal Approximation](#103-standardizing-and-normal-approximation)
- [10.4 Chi-Square, t, and F Distributions](#104-chi-square-t-and-f-distributions)
- [10.5 Delta Method Basics](#105-delta-method-basics)
- [10.6 Sampling Distribution Examples](#106-sampling-distribution-examples)
- [10.7 Continuity Corrections and Normal Approximation Tips](#107-continuity-corrections-and-normal-approximation-tips)

### [Week 11: Estimation](#week-11-estimation)

- [11.1 Point Estimators and Criteria](#111-point-estimators-and-criteria-sections-81-82)
- [11.2 Bias, Variance, and MSE](#112-bias-variance-and-mse)
- [11.3 Method of Moments (MoM)](#113-method-of-moments-mom)
- [11.4 Maximum Likelihood Estimation (MLE)](#114-maximum-likelihood-estimation-mle)
- [11.5 Fisher Information and Cramér-Rao Lower Bound (Preview)](#115-fisher-information-and-cramr-rao-lower-bound-preview)
- [11.6 Confidence Intervals (CI) Basics](#116-confidence-intervals-ci-basics)
- [11.7 Sample Size Determination](#117-sample-size-determination)
- [11.8 Sufficiency (Factorization Theorem) Preview](#118-sufficiency-factorization-theorem-preview)
- [11.9 Worked Foundational Estimation Examples](#119-worked-foundational-estimation-examples)

### [Week 12: Properties of Point Estimation and Methods of Estimation](#week-12-properties-of-point-estimation-and-methods-of-estimation)

- [12.1 Sufficiency (Formal)](#121-sufficiency-formal-sections-91-94)
- [12.2 Completeness and Lehmann-Scheffé](#122-completeness-and-lehmann-scheff)
- [12.3 Method of Moments vs MLE](#123-method-of-moments-vs-mle)
- [12.4 Fisher Information and Cramér-Rao Lower Bound (Detailed)](#124-fisher-information-and-cramr-rao-lower-bound-detailed)
- [12.5 Asymptotic Normality of MLE](#125-asymptotic-normality-of-mle)
- [12.6 Efficiency and Comparing Estimators](#126-efficiency-and-comparing-estimators)
- [12.7 Worked Advanced Estimation Examples](#127-worked-advanced-estimation-examples)

### [Week 13: Hypothesis Testing](#week-13-hypothesis-testing)

- [13.1 Framework and Definitions](#131-framework-and-definitions-sections-101-102)
- [13.2 Types of Errors](#132-types-of-errors)
- [13.3 p-Values](#133-p-values)
- [13.4 Confidence Intervals Duality](#134-confidence-intervals-duality)
- [13.5 Common Test Statistics](#135-common-test-statistics)
- [13.6 Power and Sample Size (Means)](#136-power-and-sample-size-means)
- [13.7 Multiple Testing (Brief)](#137-multiple-testing-brief)
- [13.8 Worked Examples](#138-worked-examples)

### [Week 14: Testing for Means and Variances, Likelihood Ratio Tests](#week-14-testing-for-means-and-variances-likelihood-ratio-tests)

- [14.1 Neyman-Pearson Lemma](#141-neyman-pearson-lemma-section-108)
- [14.2 General Likelihood Ratio Test (LRT)](#142-general-likelihood-ratio-test-lrt)
- [14.3 Wald, Score, and LRT Triad](#143-wald-score-and-lrt-triad)
- [14.4 Worked Examples](#144-worked-examples)
- [14.5 Power and Sample Size for LRT](#145-power-and-sample-size-for-lrt)
- [14.6 Practical Guidance](#146-practical-guidance)

### [Summary and Key Points](#summary-and-key-points)

- [Core Conceptual Framework](#core-conceptual-framework)
- [Essential Formula Reference](#essential-formula-reference)
- [Study Recommendations](#study-recommendations)

### Examples Index

**Week 1 Examples:**
- [Medical Diagnosis (Bayes' Theorem)](#example-medical-diagnosis)
- [Axiom Verification](#example-axiom-verification)

**Week 2 Examples:**
- [Total Probability Derivation](#example-total-probability-derivation)

**Week 3 Examples:**
- [Quality Control (Binomial)](#example-quality-control)
- [Binomial MGF Derivation](#example-binomial-mgf-derivation)
- [Software Testing (Geometric)](#example-software-testing)
- [Geometric Distribution Properties](#example-geometric-distribution-properties)
- [Committee Selection (Hypergeometric)](#example-committee-selection)
- [Hypergeometric Limit Theorem](#example-hypergeometric-limit-theorem)
- [Network Traffic Analysis (Poisson)](#example-network-traffic-analysis)
- [Poisson Process Theory](#example-poisson-process-theory)

**Week 4 Examples:**
- [Using Moments for Binomial Distribution](#example-1-using-moments-for-binomial-distribution)
- [Using E[X²] - (E[X])² for Poisson](#example-2-using-ex²---ex²-formula-for-poisson-distribution)
- [Bus Arrival Times (Uniform)](#example-bus-arrival-times)
- [Uniform MGF Derivation](#example-uniform-mgf-derivation)
- [SAT Scores (Normal)](#example-sat-scores)
- [Normal MGF Theory](#example-normal-mgf-theory)

**Week 5 Examples:**
- [Gamma Theory](#example-gamma-theory)
- [Beta Properties](#example-beta-properties)
- [Stock Price Analysis (Chebyshev)](#example-stock-price-analysis)
- [Website Loading Times (Markov)](#example-website-loading-times)

**Week 6 Examples:**
- [Joint Distribution Analysis](#example-joint-distribution-analysis)
- [Transformation Theory](#example-transformation-theory)

**Week 7 Examples:**

- [Bivariate Normal Distribution](#example-bivariate-normal-distribution)
- [Sum of Random Variables](#example-sum-of-random-variables)
- [Portfolio Variance](#example-portfolio-variance)

**Week 8 Examples:** *(Midterm review – see practice set in Week 8 section)*

**Week 9 Examples (Functions of RVs):**
- [Example 9.2A: Exponential Scaling](#example-92a-exponential-scaling)
- [Example 9.3A: Squared Standard Normal](#example-93a-squared-standard-normal)
- [Example 9.4A: Polar Transformation](#example-94a-polar-transformation)
- [Example 9.5A: Sum of Exponentials](#example-95a-sum-of-exponentials)
- [Example 9.6A: Cauchy from Normal Ratio](#example-96a-cauchy-from-normal-ratio)
- [Example 9.7A: Minimum Order Statistic](#example-97a-minimum-order-statistic)

**Week 10 Examples (Sampling Distributions & Limit Theorems):**

- [CLT Sample Mean Approximation](#example-clt-sample-mean-approximation)
- [Normal Approximation to Binomial](#example-normal-approximation-to-binomial)
- [Delta Method Log-Mean](#example-delta-method-log-mean)
- [Chi-Square from Normal Squares](#example-chi-square-from-normal-squares)

**Week 11 Examples (Estimation Fundamentals):**
- [Bernoulli MLE and Bias](#example-bernoulli-mle-and-bias)
- [Exponential Fisher Information](#example-exponential-fisher-information)
- [Sufficient Statistic Factorization](#example-sufficient-statistic-factorization)
- [Asymptotic Normality of Poisson MLE](#example-asymptotic-normality-of-poisson-mle)

**Week 12 Examples (Advanced Estimation Properties):**
- [Lehmann–Scheffé UMVU Example](#example-lehmannscheffé-umvu-example)

**Week 13 Examples (Hypothesis Testing):** *(Add concrete numeric test examples as needed)*

**Week 14 Examples (LRT / NP):** *(Add worked LRT derivations as needed)*

- [Bernoulli MLE and Bias](#example-bernoulli-mle-and-bias)
- [Exponential Fisher Information](#example-exponential-fisher-information)
- [Sufficient Statistic Factorization](#example-sufficient-statistic-factorization)
- [Lehmann–Scheffé UMVU Example](#example-lehmannscheffé-umvu-example)
- [Asymptotic Normality of Poisson MLE](#example-asymptotic-normality-of-poisson-mle)

---

## Week 1: Introduction to Probability

**Reading:** Wackerly et al. Sections 2.2-2.7 (36 pp.)

**Learning Objectives:**
- Understand and apply basic principles of probability
- Calculate probabilities of events based on counting points
- Understand and apply conditional probability and independence of events

[Back to Table of Contents](#table-of-contents)

### 1.1 Probabilistic Models and Sample Spaces (Section 2.2-2.3)

**Experiment**: A process that generates well-defined outcomes.

**Sample Space $S$**: The set of all possible outcomes of an experiment.
- **Discrete Sample Space:** Finite or countably infinite outcomes
- **Continuous Sample Space:** Uncountably infinite outcomes (e.g., intervals)

**Event**: Any subset of the sample space.
- **Simple Event:** Contains exactly one outcome
- **Compound Event:** Contains multiple outcomes

**Examples of Sample Spaces:**
- Coin flip: $S = \{\text{H}, \text{T}\}$
- Die roll: $S = \{1, 2, 3, 4, 5, 6\}$
- Lifetime of a lightbulb: $S = [0, \infty)$

### 1.2 Basic Probability and Axioms (Section 2.4)

**Probability** is a function that assigns numbers to events satisfying three axioms:

**Axiom 1 (Non-negativity):**
$$P(A) \geq 0$$
for any event $A$.

**Axiom 2 (Normalization):**
$$P(S) = 1$$
where $S$ is the sample space.

**Axiom 3 (Countable Additivity):**
If $A_1, A_2, A_3, \ldots$ are mutually exclusive (disjoint) events, then:
$$P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)$$

**Sample-Point Method** (Section 2.5):
When outcomes are equally likely:
$$P(A) = \frac{\text{Number of outcomes in } A}{\text{Total number of outcomes in } S}$$

### 1.3 Basic Probability Results and Rules

**Theorem: Probability of Complement**
$$P(A^c) = 1 - P(A)$$

**Proof:** Since $S = A \cup A^c$ and $A$ and $A^c$ are disjoint:
$$P(S) = P(A) + P(A^c) = 1$$
Therefore: $P(A^c) = 1 - P(A)$

**Theorem: Probability of Empty Set**
$$P(\emptyset) = 0$$

**Fundamental Probability Rules**

**Addition Rule (General Form):**
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

**Special Case - Mutually Exclusive Events:**
If $A \cap B = \emptyset$, then:
$$P(A \cup B) = P(A) + P(B)$$

### 1.4 Counting Techniques (Section 2.6)

Counting is essential for the sample-point method when outcomes are equally likely.

**Multiplication Principle:**
If one task can be performed in $n_1$ ways and a second task in $n_2$ ways, then both tasks can be performed in $n_1 \times n_2$ ways.

**Permutations:**
Number of ways to arrange $n$ distinct objects in order:
$$P_n = n!$$

Number of ways to arrange $r$ objects from $n$ distinct objects:
$$P_n^r = \frac{n!}{(n-r)!} = n(n-1)(n-2)\cdots(n-r+1)$$

**Combinations:**
Number of ways to choose $r$ objects from $n$ objects (order doesn't matter):
$$\binom{n}{r} = \frac{n!}{r!(n-r)!}$$

**Properties of Combinations:**
- $\binom{n}{r} = \binom{n}{n-r}$
- $\binom{n}{0} = \binom{n}{n} = 1$
- $\binom{n}{r} + \binom{n}{r-1} = \binom{n+1}{r}$ (Pascal's identity)

**Example: Card Hands**
Number of 5-card poker hands from a standard 52-card deck:
$$\binom{52}{5} = \frac{52!}{5!47!} = 2,598,960$$

### 1.5 Conditional Probability (Section 2.7)

**Definition:**
The conditional probability of event $A$ given event $B$ is:
$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$
provided that $P(B) > 0$.

**Multiplication Rule:**
$$P(A \cap B) = P(A|B) \cdot P(B) = P(B|A) \cdot P(A)$$

**Chain Rule:**
For events $A_1, A_2, \ldots, A_n$:
$$P(A_1 \cap A_2 \cap \cdots \cap A_n) = P(A_1) \cdot P(A_2|A_1) \cdot P(A_3|A_1 \cap A_2) \cdots P(A_n|A_1 \cap \cdots \cap A_{n-1})$$

#### Example: Medical Diagnosis
A patient shows symptoms. We know:
- $P(\text{Disease}) = 0.01$ (disease prevalence)
- $P(\text{Positive Test}|\text{Disease}) = 0.95$ (test sensitivity)
- $P(\text{Positive Test}|\text{No Disease}) = 0.05$ (false positive rate)

**Step 1:** Find $P(\text{Positive Test})$ using Law of Total Probability:
$$P(\text{Positive Test}) = P(\text{Positive}|\text{Disease}) \cdot P(\text{Disease}) + P(\text{Positive}|\text{No Disease}) \cdot P(\text{No Disease})$$
$$= 0.95 \times 0.01 + 0.05 \times 0.99 = 0.0095 + 0.0495 = 0.059$$

**Step 2:** Find $P(\text{Disease}|\text{Positive Test})$ using Bayes' Theorem:
$$P(\text{Disease}|\text{Positive}) = \frac{P(\text{Positive}|\text{Disease}) \cdot P(\text{Disease})}{P(\text{Positive})} = \frac{0.95 \times 0.01}{0.059} = \frac{0.0095}{0.059} \approx 0.161$$

This shows that even with a positive test, there's only a 16.1% chance of actually having the disease!

#### Example: Axiom Verification
Let's verify that conditional probability satisfies the probability axioms.

Given: $P(A|B) = \frac{P(A \cap B)}{P(B)}$ where $P(B) > 0$.

**Axiom 1 (Non-negativity):** $P(A|B) \geq 0$
Since $P(A \cap B) \geq 0$ and $P(B) > 0$, we have $P(A|B) = \frac{P(A \cap B)}{P(B)} \geq 0$ ✓

**Axiom 2 (Normalization):** $P(S|B) = 1$ where $S$ is the sample space
$$P(S|B) = \frac{P(S \cap B)}{P(B)} = \frac{P(B)}{P(B)} = 1$$

✓

**Axiom 3 (Countable Additivity):** For disjoint events $A_1, A_2, \ldots$:
$$P\left(\bigcup_{i=1}^{\infty} A_i \Big| B\right) = \frac{P\left(\left(\bigcup_{i=1}^{\infty} A_i\right) \cap B\right)}{P(B)} = \frac{P\left(\bigcup_{i=1}^{\infty} (A_i \cap B)\right)}{P(B)}$$

Since the $A_i$ are disjoint, so are the $A_i \cap B$:
$$= \frac{\sum_{i=1}^{\infty} P(A_i \cap B)}{P(B)} = \sum_{i=1}^{\infty} \frac{P(A_i \cap B)}{P(B)} = \sum_{i=1}^{\infty} P(A_i|B)$$

✓

This proves conditional probability forms a valid probability measure.

### 1.6 Independence

**Two Events:**
Events $A$ and $B$ are independent if and only if:
$$P(A \cap B) = P(A) \cdot P(B)$$

**Equivalent Conditions:**

- $P(A|B) = P(A)$ (when $P(B) > 0$)
- $P(B|A) = P(B)$ (when $P(A) > 0$)

**Multiple Events:**
Events $A_1, A_2, \ldots, A_n$ are mutually independent if for any subset $\{i_1, i_2, \ldots, i_k\}$:
$$P(A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{i_k}) = P(A_{i_1}) \cdot P(A_{i_2}) \cdots P(A_{i_k})$$

---

## Week 2: Some Probability Laws

**Reading:** Wackerly et al. Sections 2.8-2.12 (20 pp.)

**Learning Objectives:**
- Understand and apply multiplicative and additive laws of probability
- Understand and apply the law of total probability
- Understand and apply Bayes' rule
- Understand random variables

[Back to Table of Contents](#table-of-contents)

### 2.1 Multiplicative and Additive Laws (Section 2.8)

**Multiplicative Law of Probability:**
For any two events $A$ and $B$ with $P(B) > 0$:
$$P(A \cap B) = P(A|B) \cdot P(B)$$

**General Multiplicative Law:**
For events $A_1, A_2, \ldots, A_n$:
$$P(A_1 \cap A_2 \cap \cdots \cap A_n) = P(A_1) \cdot P(A_2|A_1) \cdot P(A_3|A_1 \cap A_2) \cdots P(A_n|A_1 \cap \cdots \cap A_{n-1})$$

**Additive Law** (already covered in Week 1):
$$P(A \cup B) = P(A) + P(B) - P(A \cap B)$$

For mutually exclusive events: $P(A \cup B) = P(A) + P(B)$

### 2.2 Law of Total Probability and Bayes' Rule (Sections 2.9-2.10)

**Law of Total Probability (Section 2.10):**
Let $B_1, B_2, \ldots, B_n$ be a partition of the sample space $S$ with $P(B_i) > 0$ for all $i$. Then for any event $A$:
$$P(A) = \sum_{i=1}^{n} P(A|B_i) \cdot P(B_i)$$

**Event-Composition Method (Section 2.9):**
This method involves:
1. Decomposing the event of interest into simpler events
2. Applying probability laws to calculate the probability
3. Often combined with law of total probability

**Event-Composition Method (Section 2.9):**
This method involves:
1. Decomposing the event of interest into simpler events
2. Applying probability laws to calculate the probability
3. Often combined with law of total probability

**Bayes' Theorem (Section 2.10):**

**Basic Form:**
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

**Complete Form (with Law of Total Probability):**
If $B_1, B_2, \ldots, B_n$ partition the sample space, then:
$$P(B_i|A) = \frac{P(A|B_i) \cdot P(B_i)}{\sum_{j=1}^{n} P(A|B_j) \cdot P(B_j)}$$

**Terminology:**
- $P(B_i)$: Prior probability (before observing $A$)
- $P(B_i|A)$: Posterior probability (after observing $A$)
- $P(A|B_i)$: Likelihood of $A$ given $B_i$
- $P(A)$: Marginal probability

**Basic Form:**
$$P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}$$

**Complete Form (with Law of Total Probability):**
$$P(B_i|A) = \frac{P(A|B_i) \cdot P(B_i)}{\sum_{j=1}^{n} P(A|B_j) \cdot P(B_j)}$$

#### Example: Medical Diagnosis (Bayes' Theorem Application)
A patient shows symptoms. We know:
- $P(\text{Disease}) = 0.01$ (disease prevalence - prior)
- $P(\text{Positive Test}|\text{Disease}) = 0.95$ (test sensitivity - likelihood)
- $P(\text{Positive Test}|\text{No Disease}) = 0.05$ (false positive rate)

**Question:** If the test is positive, what's the probability of actually having the disease?

**Step 1:** Find $P(\text{Positive Test})$ using Law of Total Probability:
$$P(\text{Positive}) = P(\text{Positive}|\text{Disease}) \cdot P(\text{Disease}) + P(\text{Positive}|\text{No Disease}) \cdot P(\text{No Disease})$$
$$= 0.95 \times 0.01 + 0.05 \times 0.99 = 0.0095 + 0.0495 = 0.059$$

**Step 2:** Find $P(\text{Disease}|\text{Positive Test})$ using Bayes' Theorem:
$$P(\text{Disease}|\text{Positive}) = \frac{P(\text{Positive}|\text{Disease}) \cdot P(\text{Disease})}{P(\text{Positive})}$$
$$= \frac{0.95 \times 0.01}{0.059} = \frac{0.0095}{0.059} \approx 0.161$$

**Conclusion:** Even with a positive test, there's only a 16.1% chance of actually having the disease! This counterintuitive result illustrates the importance of base rates (prior probabilities).

### 2.3 Numerical Events and Random Variables (Section 2.11)

**Numerical Event**: An event that can be described in terms of the value of a numerical quantity.

**Random Variable**: A function that assigns a numerical value to each outcome in the sample space.
$$X: S \to \mathbb{R}$$

**Why Random Variables?**
- Allow us to work with numerical outcomes
- Enable use of calculus and analytical methods
- Simplify probability calculations

**Types of Random Variables:**

- **Discrete Random Variable:** Takes values in a countable set (finite or countably infinite)
  - Examples: Number of heads in coin flips, number of defects in a product
  
- **Continuous Random Variable:** Takes values in an uncountable set (typically an interval)
  - Examples: Height, weight, time until failure

**Cumulative Distribution Function (CDF):**
$$F_X(x) = P(X \leq x)$$

**Properties of CDF:**
- Monotonically non-decreasing
- Right-continuous
- $\lim_{x \to -\infty} F_X(x) = 0$
- $\lim_{x \to +\infty} F_X(x) = 1$

**Events Involving Random Variables:**
- $\{X = x\}$: The event that random variable $X$ equals $x$
- $\{X \leq x\}$: The event that $X$ is at most $x$
- $\{a < X \leq b\}$: The event that $X$ falls in the interval $(a, b]$

### 2.4 Random Sampling (Section 2.12)

**Random Sample**: A collection of observations $X_1, X_2, \ldots, X_n$ drawn from a population such that:
1. Each $X_i$ has the same probability distribution
2. The $X_i$ are mutually independent

**Notation:** Often written as "i.i.d." (independent and identically distributed)

**Sampling With Replacement:** Each selection is made from the complete population
- Results in independent observations
- Each observation has the same distribution

**Sampling Without Replacement:** Each selection removes an element from the population
- Results in dependent observations (unless population is very large relative to sample)
- When sample size is small relative to population, approximately independent

**Example: Quality Control Sampling**
A box contains 100 items, 10 of which are defective.
- **With replacement:** Each draw has $P(\text{defective}) = 0.1$, selections are independent
- **Without replacement:** First draw $P(\text{defective}) = 0.1$, but second draw probability depends on first result

**Important Note:** Most statistical theory assumes random sampling (i.i.d. observations). This assumption is crucial for:
- Law of Large Numbers
- Central Limit Theorem  
- Properties of estimators

---

## Week 3: Discrete Random Variables

**Reading:** Wackerly et al. Sections 3.1-3.5, 3.7, 3.8 (41 pp.)

**Learning Objectives:**
- Understand and apply basic calculations using discrete random variables
- Calculate expected values of discrete random variables
- Make probability calculations based on random variables having binomial, geometric, hypergeometric, or Poisson distributions

**Note:** Moments and moment generating functions (Section 3.9) will be covered in Week 4.

[Back to Table of Contents](#table-of-contents)

### 3.1 Discrete Random Variables Fundamentals (Sections 3.1-3.3)

**Probability Mass Function (PMF):**
$$p_X(x) = P(X = x)$$

**Properties:**

- $p_X(x) \geq 0$ for all $x$
- $\sum_{\text{all } x} p_X(x) = 1$

**Expected Value:**
$$E[X] = \sum_{\text{all } x} x \cdot p_X(x)$$

**Variance:**
$$\text{Var}(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2$$

**Standard Deviation:**
$$\sigma_X = \sqrt{\text{Var}(X)}$$

### 3.2 Binomial Distribution

**Notation:** $X \sim \text{Binomial}(n, p)$ or $X \sim B(n, p)$

**Probability Mass Function:**
$$P(X = k) = \binom{n}{k} p^k (1-p)^{n-k}$$
where $k = 0, 1, 2, \ldots, n$ and $\binom{n}{k} = \frac{n!}{k!(n-k)!}$

**Mathematical Derivation:**
For $n$ independent Bernoulli trials with success probability $p$, the probability of exactly $k$ successes is:

- Choose $k$ positions for successes: $\binom{n}{k}$ ways
- Probability of $k$ successes: $p^k$
- Probability of $(n-k)$ failures: $(1-p)^{n-k}$

**Expected Value and Variance:**
$$E[X] = np$$
$$\text{Var}(X) = np(1-p)$$

**Moment Generating Function:**
$$M_X(t) = (pe^t + 1-p)^n$$

#### Example: Quality Control
A factory produces items with a 5% defect rate. If we randomly select 20 items, what's the probability of finding exactly 2 defects?

Given: $n = 20$, $p = 0.05$, $k = 2$
$$P(X = 2) = \binom{20}{2} (0.05)^2 (0.95)^{18}$$
$$= \frac{20!}{2!18!} \times 0.0025 \times (0.95)^{18}$$
$$= 190 \times 0.0025 \times 0.3972 = 0.189$$

**Expected defects:** $E[X] = np = 20 \times 0.05 = 1$
**Variance:** $\text{Var}(X) = np(1-p) = 20 \times 0.05 \times 0.95 = 0.95$

#### Example: Binomial MGF Derivation
Let's derive the moment generating function for the binomial distribution from first principles.

**Given:** $X \sim \text{Binomial}(n, p)$

**Step 1:** Start with the definition
$$M_X(t) = E[e^{tX}] = \sum_{k=0}^{n} e^{tk} \cdot P(X = k) = \sum_{k=0}^{n} e^{tk} \binom{n}{k} p^k (1-p)^{n-k}$$

**Step 2:** Rearrange terms
$$M_X(t) = \sum_{k=0}^{n} \binom{n}{k} (pe^t)^k (1-p)^{n-k}$$

**Step 3:** Recognize the binomial theorem
The binomial theorem states: $(a + b)^n = \sum_{k=0}^{n} \binom{n}{k} a^k b^{n-k}$

With $a = pe^t$ and $b = 1-p$:
$$M_X(t) = (pe^t + 1-p)^n$$

**Step 4:** Verify by taking derivatives
- $M'_X(t) = n(pe^t + 1-p)^{n-1} \cdot pe^t$
- $M'_X(0) = n(p + 1-p)^{n-1} \cdot p = np$ ✓

- $M''_X(t) = n(n-1)(pe^t + 1-p)^{n-2} \cdot (pe^t)^2 + n(pe^t + 1-p)^{n-1} \cdot pe^t$
- $M''_X(0) = n(n-1)p^2 + np = n^2p^2 - np^2 + np$
- $\text{Var}(X) = n^2p^2 - np^2 + np - (np)^2 = np(1-p)$ ✓

### 3.3 Geometric Distribution

**Notation:** $X \sim \text{Geometric}(p)$

**Probability Mass Function:**
$$P(X = k) = (1-p)^{k-1} p, \quad k = 1, 2, 3, \ldots$$

**Mathematical Derivation:**
$X$ represents the trial number of the first success:

- First $(k-1)$ trials are failures: $(1-p)^{k-1}$
- $k$-th trial is a success: $p$

**Expected Value and Variance:**
$$E[X] = \frac{1}{p}$$
$$\text{Var}(X) = \frac{1-p}{p^2}$$

**Memoryless Property:**
$$P(X > m + n | X > m) = P(X > n)$$

**Moment Generating Function:**
$$M_X(t) = \frac{pe^t}{1-(1-p)e^t}$$
for $t < -\ln(1-p)$

#### Example: Software Testing
A software tester knows that each test run has a 20% chance of finding a bug. What's the probability that the first bug is found on the 5th test?

Given: $p = 0.2$, $k = 5$
$$P(X = 5) = (1-0.2)^{5-1} \times 0.2 = (0.8)^4 \times 0.2 = 0.4096 \times 0.2 = 0.08192$$

**Expected number of tests:** $E[X] = \frac{1}{0.2} = 5$
**Variance:** $\text{Var}(X) = \frac{1-0.2}{(0.2)^2} = \frac{0.8}{0.04} = 20$

**Memoryless property example:** If no bug has been found in the first 3 tests, the probability that it takes more than 5 additional tests is the same as the probability that it takes more than 5 tests from the beginning:
$$P(X > 8 | X > 3) = P(X > 5) = (0.8)^5 = 0.3277$$

#### Example: Geometric Distribution Properties
Let's prove the memoryless property mathematically and derive the variance.

**Memoryless Property Proof:**
We need to show: $P(X > m + n | X > m) = P(X > n)$

**Step 1:** Use the definition of conditional probability
$$P(X > m + n | X > m) = \frac{P(X > m + n \cap X > m)}{P(X > m)} = \frac{P(X > m + n)}{P(X > m)}$$

**Step 2:** Calculate $P(X > k)$ for geometric distribution
$$P(X > k) = \sum_{j=k+1}^{\infty} (1-p)^{j-1}p = p(1-p)^k \sum_{i=0}^{\infty} (1-p)^i = p(1-p)^k \cdot \frac{1}{1-(1-p)} = (1-p)^k$$

**Step 3:** Substitute and simplify
$$P(X > m + n | X > m) = \frac{(1-p)^{m+n}}{(1-p)^m} = (1-p)^n = P(X > n)$$

✓

**Variance Derivation using MGF:**
From $M_X(t) = \frac{pe^t}{1-(1-p)e^t}$, let $q = 1-p$.

**Step 1:** First derivative
$$M'_X(t) = \frac{pe^t[1-qe^t] - pe^t(-qe^t)}{[1-qe^t]^2} = \frac{pe^t}{[1-qe^t]^2}$$

**Step 2:** Second derivative (using quotient rule)
$$M''_X(t) = \frac{pe^t[1-qe^t]^2 - pe^t \cdot 2[1-qe^t](-qe^t)}{[1-qe^t]^4} = \frac{pe^t(1+qe^t)}{[1-qe^t]^3}$$

**Step 3:** Evaluate at $t = 0$
- $M'_X(0) = \frac{p}{(1-q)^2} = \frac{p}{p^2} = \frac{1}{p}$
- $M''_X(0) = \frac{p(1+q)}{(1-q)^3} = \frac{p(2-p)}{p^3} = \frac{2-p}{p^2}$

**Step 4:** Calculate variance
$$\text{Var}(X) = M''_X(0) - (M'_X(0))^2 = \frac{2-p}{p^2} - \frac{1}{p^2} = \frac{1-p}{p^2}$$

✓

### 3.4 Hypergeometric Distribution

**Notation:** $X \sim \text{Hypergeometric}(N, K, n)$

**Probability Mass Function:**
$$P(X = k) = \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}}$$
where $\max(0, n-(N-K)) \leq k \leq \min(n, K)$

**Mathematical Derivation:**
Population of $N$ items with $K$ "success" items. Drawing $n$ items without replacement, $X$ is the number of success items:

- Ways to choose $k$ successes from $K$: $\binom{K}{k}$
- Ways to choose $(n-k)$ failures from $(N-K)$: $\binom{N-K}{n-k}$
- Total ways to choose $n$ items: $\binom{N}{n}$

**Expected Value and Variance:**
$$E[X] = n \cdot \frac{K}{N}$$
$$\text{Var}(X) = n \cdot \frac{K}{N} \cdot \left(1-\frac{K}{N}\right) \cdot \frac{N-n}{N-1}$$

**Relationship to Binomial:** As $N \to \infty$ and $\frac{K}{N} \to p$, hypergeometric approaches binomial.

#### Example: Committee Selection
From a group of 30 people (18 women, 12 men), a committee of 8 is chosen randomly. What's the probability that exactly 5 are women?

Given: $N = 30$, $K = 18$ (women), $n = 8$, $k = 5$
$$P(X = 5) = \frac{\binom{18}{5} \binom{12}{3}}{\binom{30}{8}}$$

Calculating each term:
- $\binom{18}{5} = \frac{18!}{5!13!} = 8,568$
- $\binom{12}{3} = \frac{12!}{3!9!} = 220$
- $\binom{30}{8} = \frac{30!}{8!22!} = 5,852,925$

$$P(X = 5) = \frac{8,568 \times 220}{5,852,925} = \frac{1,884,960}{5,852,925} \approx 0.322$$

**Expected women:** $E[X] = 8 \times \frac{18}{30} = 8 \times 0.6 = 4.8$

#### Example: Hypergeometric Limit Theorem
Let's prove that hypergeometric distribution approaches binomial as $N \to \infty$ while keeping $\frac{K}{N} \to p$.

**Setup:** $X \sim \text{Hypergeometric}(N, K, n)$ where $K = Np + o(\sqrt{N})$

**Step 1:** Write the hypergeometric PMF
$$P(X = k) = \frac{\binom{K}{k}\binom{N-K}{n-k}}{\binom{N}{n}}$$

**Step 2:** Express using factorials and apply Stirling's approximation
$$\binom{N}{n} \approx \frac{N^n}{n!} \text{ as } N \to \infty \text{ for fixed } n$$

**Step 3:** For the numerator, with $K = Np$:
$$\binom{K}{k} \approx \frac{(Np)^k}{k!} \quad \text{and} \quad \binom{N-K}{n-k} \approx \frac{(N(1-p))^{n-k}}{(n-k)!}$$

**Step 4:** Combine and simplify
$$P(X = k) \approx \frac{\frac{(Np)^k}{k!} \cdot \frac{(N(1-p))^{n-k}}{(n-k)!}}{\frac{N^n}{n!}} = \frac{n!}{k!(n-k)!} \cdot \frac{(Np)^k(N(1-p))^{n-k}}{N^n}$$

$$= \binom{n}{k} p^k (1-p)^{n-k}$$

This proves the limit theorem: $\lim_{N \to \infty} \text{Hypergeometric}(N, Np, n) = \text{Binomial}(n, p)$

**Intuition:** When the population is very large relative to the sample size, sampling without replacement becomes approximately equivalent to sampling with replacement.

### 3.5 Poisson Distribution

**Notation:** $X \sim \text{Poisson}(\lambda)$

**Probability Mass Function:**
$$P(X = k) = \frac{\lambda^k e^{-\lambda}}{k!}, \quad k = 0, 1, 2, \ldots$$

**Mathematical Derivation:**
Poisson distribution is the limit of binomial distribution as $n \to \infty$, $p \to 0$, but $np = \lambda$ remains constant:
$$\lim_{n \to \infty} \binom{n}{k} p^k (1-p)^{n-k} = \frac{\lambda^k e^{-\lambda}}{k!}$$

**Expected Value and Variance:**
$$E[X] = \lambda$$
$$\text{Var}(X) = \lambda$$

**Moment Generating Function:**
$$M_X(t) = e^{\lambda(e^t - 1)}$$

**Poisson Process Properties:**

- Independent increments
- Stationary increments
- Rare events accumulation

#### Example: Network Traffic Analysis
A network server receives requests at an average rate of 3 requests per minute. What's the probability of receiving exactly 5 requests in a 2-minute interval?

First, find the parameter: $\lambda = 3 \times 2 = 6$ requests in 2 minutes.

$$P(X = 5) = \frac{6^5 e^{-6}}{5!} = \frac{7776 \times 0.002479}{120} = \frac{19.27}{120} \approx 0.161$$

**Additional calculations:**
- $P(X = 0) = \frac{6^0 e^{-6}}{0!} = e^{-6} \approx 0.0025$ (probability of no requests)
- $P(X \leq 3) = \sum_{k=0}^{3} \frac{6^k e^{-6}}{k!} \approx 0.151$ (probability of 3 or fewer requests)

**Expected requests:** $E[X] = 6$
**Standard deviation:** $\sigma = \sqrt{6} \approx 2.45$

#### Example: Poisson Process Theory
Let's derive the Poisson distribution as the limit of binomial distribution and prove that the sum of independent Poisson random variables is also Poisson.

**Part 1: Binomial → Poisson Limit**
Consider $X_n \sim \text{Binomial}(n, p_n)$ where $np_n \to \lambda$ as $n \to \infty$ and $p_n \to 0$.

**Step 1:** Write the binomial PMF
$$P(X_n = k) = \binom{n}{k} p_n^k (1-p_n)^{n-k}$$

**Step 2:** Substitute $p_n = \frac{\lambda + \epsilon_n}{n}$ where $\epsilon_n \to 0$
$$P(X_n = k) = \frac{n!}{k!(n-k)!} \left(\frac{\lambda + \epsilon_n}{n}\right)^k \left(1-\frac{\lambda + \epsilon_n}{n}\right)^{n-k}$$

**Step 3:** Simplify the combinatorial term
$$\frac{n!}{(n-k)!n^k} = \frac{n(n-1)\cdots(n-k+1)}{n^k} \to 1 \text{ as } n \to \infty$$

**Step 4:** Handle the exponential terms
$$\left(1-\frac{\lambda + \epsilon_n}{n}\right)^n \to e^{-\lambda}$$
$$\left(1-\frac{\lambda + \epsilon_n}{n}\right)^{-k} \to 1$$

**Step 5:** Combine results
$$\lim_{n \to \infty} P(X_n = k) = \frac{\lambda^k}{k!} \cdot e^{-\lambda} = \frac{\lambda^k e^{-\lambda}}{k!}$$

**Part 2: Sum Property**
If $X \sim \text{Poisson}(\lambda_1)$ and $Y \sim \text{Poisson}(\lambda_2)$ are independent, then $Z = X + Y \sim \text{Poisson}(\lambda_1 + \lambda_2)$.

**Proof using MGFs:**
- $M_X(t) = e^{\lambda_1(e^t - 1)}$
- $M_Y(t) = e^{\lambda_2(e^t - 1)}$
- $M_Z(t) = M_X(t) \cdot M_Y(t) = e^{\lambda_1(e^t - 1)} \cdot e^{\lambda_2(e^t - 1)} = e^{(\lambda_1 + \lambda_2)(e^t - 1)}$

This is the MGF of $\text{Poisson}(\lambda_1 + \lambda_2)$, proving the result. ✓

---

## Week 4: Moments, Moment Generating Functions, and Continuous Random Variables

**Reading:** Wackerly et al. Sections 3.9, 4.1-4.5 (26 pp.)

**Learning Objectives:**
- Derive and apply moment generating functions for a given distribution
- Understand and apply basic calculations using continuous random variables
- Calculate expected values of continuous random variables
- Make probability calculations based on random variables having uniform or normal distributions

[Back to Table of Contents](#table-of-contents)

### 4.1 Moments and Moment Generating Functions (Section 3.9)

**$k$-th Raw Moment:**
$$\mu'_k = E[X^k]$$

**$k$-th Central Moment:**
$$\mu_k = E[(X - \mu)^k], \quad \text{where } \mu = E[X]$$

**Important Moments:**

- First raw moment: $\mu'_1 = E[X] = \mu$ (mean)
- Second central moment: $\mu_2 = E[(X-\mu)^2] = \text{Var}(X)$ (variance)
- Third central moment: $\mu_3 = E[(X-\mu)^3]$ (measure of skewness)
- Fourth central moment: $\mu_4 = E[(X-\mu)^4]$ (measure of kurtosis)

**Relationship Between Moments and Variance:**

The fundamental relationship between raw moments and variance is:
$$\text{Var}(X) = E[X^2] - (E[X])^2 = \mu'_2 - (\mu'_1)^2$$

**Derivation:**
$$\text{Var}(X) = E[(X - \mu)^2] = E[X^2 - 2\mu X + \mu^2]$$
$$= E[X^2] - 2\mu E[X] + \mu^2$$
$$= E[X^2] - 2\mu^2 + \mu^2$$
$$= E[X^2] - \mu^2$$
$$= E[X^2] - (E[X])^2$$

This formula is particularly useful for calculating variance when we know the first and second raw moments.

**Moment Generating Function (MGF):**
$$M_X(t) = E[e^{tX}]$$

**Existence:** MGF exists if there exists a positive constant $a$ such that $M_X(t)$ is finite for all $t \in [-a, a]$.

**Key Properties:**

1. **Uniqueness:** MGF uniquely determines the distribution
2. **Moment Generation:** $\mu'_k = M_X^{(k)}(0)$ (k-th derivative at $t=0$)
3. **Independence:** If $X, Y$ independent, then $M_{X+Y}(t) = M_X(t) \cdot M_Y(t)$

**Taylor Expansion:**
$$M_X(t) = E[e^{tX}] = E\left[\sum_{k=0}^{\infty} \frac{(tX)^k}{k!}\right] = \sum_{k=0}^{\infty} \frac{E[X^k] \cdot t^k}{k!}$$

**Using MGF to Find Mean and Variance:**

The moment generating function provides an elegant method to find moments:

1. **Mean (First Moment):**
   $$E[X] = M'_X(0) = \left.\frac{d}{dt}M_X(t)\right|_{t=0}$$

2. **Second Raw Moment:**
   $$E[X^2] = M''_X(0) = \left.\frac{d^2}{dt^2}M_X(t)\right|_{t=0}$$

3. **Variance:**
   $$\text{Var}(X) = M''_X(0) - (M'_X(0))^2 = E[X^2] - (E[X])^2$$

**General Formula for k-th Moment:**
$$E[X^k] = M_X^{(k)}(0)$$
where $M_X^{(k)}(0)$ denotes the k-th derivative of $M_X(t)$ evaluated at $t=0$.

#### Example 1: Using Moments for Binomial Distribution

Consider $X \sim \text{Binomial}(n, p)$ with MGF $M_X(t) = (pe^t + 1-p)^n$.

**Finding the Mean:**
$$M'_X(t) = n(pe^t + 1-p)^{n-1} \cdot pe^t$$
$$E[X] = M'_X(0) = n(p + 1-p)^{n-1} \cdot p = np$$

**Finding the Variance:**
Using the product rule:
$$M''_X(t) = n(n-1)(pe^t + 1-p)^{n-2} \cdot (pe^t)^2 + n(pe^t + 1-p)^{n-1} \cdot pe^t$$

At $t = 0$:
$$M''_X(0) = n(n-1)p^2 + np = n^2p^2 - np^2 + np$$

Therefore:
$$\text{Var}(X) = M''_X(0) - (M'_X(0))^2 = n^2p^2 - np^2 + np - (np)^2$$
$$= n^2p^2 - np^2 + np - n^2p^2 = np - np^2 = np(1-p)$$

#### Example 2: Using E[X²] - (E[X])² Formula for Poisson Distribution

For $X \sim \text{Poisson}(\lambda)$:
- $E[X] = \lambda$
- $E[X^2] = E[X(X-1)] + E[X] = \lambda + \lambda = \lambda + \lambda = \lambda^2 + \lambda$

Therefore:
$$\text{Var}(X) = E[X^2] - (E[X])^2 = (\lambda^2 + \lambda) - \lambda^2 = \lambda$$

**Verification using MGF:** For Poisson with $M_X(t) = e^{\lambda(e^t - 1)}$:
- $M'_X(t) = \lambda e^t \cdot e^{\lambda(e^t - 1)}$, so $M'_X(0) = \lambda$
- $M''_X(t) = \lambda e^t \cdot e^{\lambda(e^t - 1)} \cdot (\lambda e^t + 1)$, so $M''_X(0) = \lambda(\lambda + 1) = \lambda^2 + \lambda$
- $\text{Var}(X) = (\lambda^2 + \lambda) - \lambda^2 = \lambda$ ✓

### 4.2 Continuous Random Variables Fundamentals

**Probability Density Function (PDF):**
$$f_X(x) \geq 0 \quad \text{and} \quad \int_{-\infty}^{\infty} f_X(x) \, dx = 1$$

**Relationship between CDF and PDF:**
$$F_X(x) = \int_{-\infty}^{x} f_X(t) \, dt$$
$$f_X(x) = \frac{dF_X(x)}{dx} \quad \text{(at points of continuity)}$$

**Probability Calculations:**
$$P(a < X \leq b) = \int_a^b f_X(x) \, dx = F_X(b) - F_X(a)$$

**Expected Value:**
$$E[X] = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx$$

**Variance:**
$$\text{Var}(X) = \int_{-\infty}^{\infty} (x - E[X])^2 \cdot f_X(x) \, dx = E[X^2] - (E[X])^2$$

### 4.3 Uniform Distribution

**Notation:** $X \sim \text{Uniform}(a, b)$ or $X \sim U(a, b)$

**Probability Density Function:**
$$f_X(x) = \begin{cases}
\frac{1}{b-a}, & \text{if } a \leq x \leq b \\
0, & \text{otherwise}
\end{cases}$$

**Cumulative Distribution Function:**
$$F_X(x) = \begin{cases}
0, & \text{if } x < a \\
\frac{x-a}{b-a}, & \text{if } a \leq x < b \\
1, & \text{if } x \geq b
\end{cases}$$

**Expected Value and Variance:**
$$E[X] = \frac{a + b}{2}$$
$$\text{Var}(X) = \frac{(b - a)^2}{12}$$

**Moment Generating Function Derivation:**
$$M_X(t) = E[e^{tX}] = \int_a^b e^{tx} \cdot \frac{1}{b-a} \, dx$$

For $t \neq 0$:
$$M_X(t) = \frac{1}{b-a} \cdot \left[\frac{e^{tx}}{t}\right]_a^b = \frac{e^{tb} - e^{ta}}{t(b-a)}$$

For $t = 0$:
$$M_X(0) = 1$$

**Special Handling:** At $t=0$, the MGF has a removable discontinuity requiring limit evaluation.

#### Example: Bus Arrival Times
Bus arrivals are uniformly distributed between 0 and 15 minutes. If you arrive at the bus stop at a random time, what's the probability you wait between 5 and 10 minutes?

Given: $X \sim \text{Uniform}(0, 15)$

$$P(5 \leq X \leq 10) = \int_5^{10} \frac{1}{15} dx = \frac{1}{15} \times (10 - 5) = \frac{5}{15} = \frac{1}{3} \approx 0.333$$

Using CDF: $P(5 \leq X \leq 10) = F(10) - F(5) = \frac{10}{15} - \frac{5}{15} = \frac{1}{3}$

**Expected waiting time:** $E[X] = \frac{0 + 15}{2} = 7.5$ minutes
**Variance:** $\text{Var}(X) = \frac{(15-0)^2}{12} = \frac{225}{12} = 18.75$

#### Example: Uniform MGF Derivation
Let's derive the complete MGF for the uniform distribution and handle the discontinuity at $t = 0$.

**Given:** $X \sim \text{Uniform}(a, b)$

**Step 1:** Set up the integral
$$M_X(t) = E[e^{tX}] = \int_a^b e^{tx} \cdot \frac{1}{b-a} dx$$

**Step 2:** Evaluate for $t \neq 0$
$$M_X(t) = \frac{1}{b-a} \int_a^b e^{tx} dx = \frac{1}{b-a} \left[\frac{e^{tx}}{t}\right]_a^b = \frac{e^{tb} - e^{ta}}{t(b-a)}$$

**Step 3:** Handle the case $t = 0$
Direct substitution gives $\frac{0}{0}$, so we use L'Hôpital's rule:
$$M_X(0) = \lim_{t \to 0} \frac{e^{tb} - e^{ta}}{t(b-a)} = \lim_{t \to 0} \frac{be^{tb} - ae^{ta}}{b-a} = \frac{b - a}{b-a} = 1$$

**Step 4:** Complete MGF
$$M_X(t) = \begin{cases}
\frac{e^{tb} - e^{ta}}{t(b-a)}, & \text{if } t \neq 0 \\
1, & \text{if } t = 0
\end{cases}$$

**Step 5:** Verify moments
Using L'Hôpital's rule twice for the first moment:
$$E[X] = M'_X(0) = \lim_{t \to 0} \frac{d}{dt}\left[\frac{e^{tb} - e^{ta}}{t(b-a)}\right]$$

After applying L'Hôpital's rule:
$$= \lim_{t \to 0} \frac{b^2e^{tb} - a^2e^{ta}}{2(b-a)} = \frac{b^2 - a^2}{2(b-a)} = \frac{(b-a)(b+a)}{2(b-a)} = \frac{a+b}{2}$$

✓

### 4.4 Exponential Distribution

**Notation:** $X \sim \text{Exponential}(\beta)$ where $\beta > 0$ is the scale parameter (mean)

**Probability Density Function:**
$$f_X(x) = \frac{1}{\beta} e^{-x/\beta}, \quad x > 0$$

**Cumulative Distribution Function:**
$$F_X(x) = 1 - e^{-x/\beta}, \quad x > 0$$

**Expected Value and Variance:**
$$E[X] = \beta$$
$$\text{Var}(X) = \beta^2$$

**Moment Generating Function:**
$$M_X(t) = \frac{1}{1 - \beta t} = (1 - \beta t)^{-1}, \quad t < \frac{1}{\beta}$$

**Memoryless Property:**
The exponential distribution is the only continuous distribution with the memoryless property:
$$P(X > s + t | X > s) = P(X > t)$$

**Relationship to Gamma:**
$\text{Exponential}(\beta) = \text{Gamma}(1, \beta)$ - it is a special case of the Gamma distribution with shape parameter $\alpha = 1$.

**Applications:**
- Waiting times between events in a Poisson process
- Lifetime/reliability analysis
- Radioactive decay

#### Example: Exponential Memoryless Property

Let $X \sim \text{Exponential}(\beta)$. Verify the memoryless property:

**Step 1:** Calculate $P(X > x)$
$$P(X > x) = 1 - F_X(x) = 1 - (1 - e^{-x/\beta}) = e^{-x/\beta}$$

**Step 2:** Calculate conditional probability
$$P(X > s + t | X > s) = \frac{P(X > s + t)}{P(X > s)} = \frac{e^{-(s+t)/\beta}}{e^{-s/\beta}} = e^{-t/\beta} = P(X > t)$$

This proves the memoryless property.

### 4.5 Normal Distribution

**Notation:** $X \sim \text{Normal}(\mu, \sigma^2)$ or $X \sim N(\mu, \sigma^2)$

**Probability Density Function:**
$$f_X(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]$$

**Standard Normal Distribution:** $Z \sim N(0, 1)$
$$\phi(z) = \frac{1}{\sqrt{2\pi}} e^{-z^2/2}$$
$$\Phi(z) = \int_{-\infty}^z \phi(t) \, dt$$

**Standardization:**
If $X \sim N(\mu, \sigma^2)$, then $Z = \frac{X - \mu}{\sigma} \sim N(0, 1)$

**Expected Value and Variance:**
$$E[X] = \mu$$
$$\text{Var}(X) = \sigma^2$$

**Moment Generating Function Derivation:**
$$M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} \cdot \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] dx$$

**Completing the Square:**
Combine the exponential terms:
$$tx - \frac{(x-\mu)^2}{2\sigma^2} = -\frac{1}{2\sigma^2}\left[x^2 - 2x(\mu + \sigma^2t) + \mu^2\right]$$

$$= -\frac{1}{2\sigma^2}\left[(x - (\mu + \sigma^2t))^2 - (\mu + \sigma^2t)^2 + \mu^2\right]$$

$$= -\frac{(x - (\mu + \sigma^2t))^2}{2\sigma^2} + \mu t + \frac{\sigma^2t^2}{2}$$

**Final Result:**
$$M_X(t) = \exp\left(\mu t + \frac{\sigma^2t^2}{2}\right)$$

**68-95-99.7 Rule:**
- $P(\mu - \sigma \leq X \leq \mu + \sigma) \approx 0.68$
- $P(\mu - 2\sigma \leq X \leq \mu + 2\sigma) \approx 0.95$
- $P(\mu - 3\sigma \leq X \leq \mu + 3\sigma) \approx 0.997$

#### Example: SAT Scores
SAT scores are normally distributed with $\mu = 500$ and $\sigma = 100$.

**Question 1:** What percentage of students score between 400 and 600?
Using the 68-95-99.7 rule: Since 400 and 600 are exactly 1 standard deviation from the mean, approximately 68% of students score in this range.

**Question 2:** What's the probability a student scores above 650?
First, standardize: $Z = \frac{650 - 500}{100} = 1.5$
$$P(X > 650) = P(Z > 1.5) = 1 - \Phi(1.5) = 1 - 0.9332 = 0.0668$$

**Question 3:** What score is exceeded by only 10% of students?
Find the 90th percentile: $P(X \leq x) = 0.90$
From standard normal table: $\Phi(1.28) = 0.90$
So: $\frac{x - 500}{100} = 1.28 \Rightarrow x = 500 + 128 = 628$

**Question 4:** Using MGF to verify mean and variance
For $X \sim N(500, 100^2)$, the MGF is:
$$M_X(t) = \exp(500t + \frac{10000t^2}{2}) = \exp(500t + 5000t^2)$$

- $M'_X(t) = (500 + 10000t)\exp(500t + 5000t^2)$, so $M'_X(0) = 500$ ✓
- $M''_X(t) = [10000 + (500 + 10000t)^2]\exp(500t + 5000t^2)$, so $M''_X(0) = 10000 + 500^2 = 260000$
- $\text{Var}(X) = 260000 - 500^2 = 260000 - 250000 = 10000 = 100^2$ ✓

#### Example: Normal MGF Theory
Let's derive the normal distribution MGF from first principles and prove the reproductive property.

**Part 1: MGF Derivation for $N(\mu, \sigma^2)$**

**Step 1:** Start with the definition
$$M_X(t) = E[e^{tX}] = \int_{-\infty}^{\infty} e^{tx} \cdot \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right] dx$$

**Step 2:** Combine exponentials
$$M_X(t) = \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp\left[tx - \frac{(x-\mu)^2}{2\sigma^2}\right] dx$$

**Step 3:** Complete the square in the exponent
$$tx - \frac{(x-\mu)^2}{2\sigma^2} = -\frac{1}{2\sigma^2}\left[(x-\mu)^2 - 2\sigma^2tx\right]$$

Let $u = x - \mu$, then:
$$= -\frac{1}{2\sigma^2}\left[u^2 - 2\sigma^2t(u + \mu)\right] = -\frac{1}{2\sigma^2}\left[u^2 - 2\sigma^2tu\right] + \mu t$$

Complete the square: $u^2 - 2\sigma^2tu = (u - \sigma^2t)^2 - \sigma^4t^2$

**Step 4:** Substitute back
$$M_X(t) = e^{\mu t} \cdot \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp\left[-\frac{(u - \sigma^2t)^2}{2\sigma^2} + \frac{\sigma^2t^2}{2}\right] du$$

$$= e^{\mu t + \frac{\sigma^2t^2}{2}} \cdot \frac{1}{\sigma\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp\left[-\frac{(u - \sigma^2t)^2}{2\sigma^2}\right] du$$

**Step 5:** Recognize the normal integral
The integral equals $\sigma\sqrt{2\pi}$ (integrating a normal density), so:
$$M_X(t) = \exp\left(\mu t + \frac{\sigma^2t^2}{2}\right)$$

**Part 2: Reproductive Property**
If $X_1 \sim N(\mu_1, \sigma_1^2)$ and $X_2 \sim N(\mu_2, \sigma_2^2)$ are independent, prove $X_1 + X_2 \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$.

**Proof using MGFs:**
$$M_{X_1 + X_2}(t) = M_{X_1}(t) \cdot M_{X_2}(t)$$
$$= \exp\left(\mu_1 t + \frac{\sigma_1^2t^2}{2}\right) \cdot \exp\left(\mu_2 t + \frac{\sigma_2^2t^2}{2}\right)$$
$$= \exp\left((\mu_1 + \mu_2)t + \frac{(\sigma_1^2 + \sigma_2^2)t^2}{2}\right)$$

This is the MGF of $N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$, completing the proof. ✓

### 4.6 Important Theorems and Properties

**Continuity Correction:**
When approximating discrete distributions with continuous ones:
$$P(X = k) \approx P(k - 0.5 < Y < k + 0.5)$$

**Linear Transformation Properties:**
If $Y = aX + b$, then:
$$E[Y] = aE[X] + b$$
$$\text{Var}(Y) = a^2\text{Var}(X)$$
$$M_Y(t) = e^{bt} \cdot M_X(at)$$

**Reproductive Property of Normal Distribution:**
If $X_1 \sim N(\mu_1, \sigma_1^2)$ and $X_2 \sim N(\mu_2, \sigma_2^2)$ are independent, then:
$$X_1 + X_2 \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$$

---

## Week 5: Some Continuous Random Variables and Chebyshev's Theorem

**Reading:** Wackerly et al. Sections 4.6-4.10 (25 pp.)

**Learning Objectives:**
- Make probability calculations based on random variables having gamma or beta distributions
- Derive and apply moment generating functions for continuous distributions
- Understand and apply Chebyshev's Theorem

**Note:** Chi-square distribution is a special case of Gamma distribution covered in Section 4.6.

[Back to Table of Contents](#table-of-contents)

### 5.1 Gamma Distribution (Section 4.6)

**Notation:** $X \sim \text{Gamma}(\alpha, \beta)$

**Probability Density Function:**
$$f_X(x) = \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta}, \quad x > 0$$

where $\alpha > 0$ (shape parameter) and $\beta > 0$ (scale parameter).

**Gamma Function:**
$$\Gamma(\alpha) = \int_0^{\infty} t^{\alpha-1} e^{-t} dt$$

**Properties:**
- $\Gamma(n) = (n-1)!$ for positive integers $n$
- $\Gamma(\alpha + 1) = \alpha \Gamma(\alpha)$
- $\Gamma(1/2) = \sqrt{\pi}$

**Expected Value and Variance:**
$$E[X] = \alpha\beta$$
$$\text{Var}(X) = \alpha\beta^2$$

**Moment Generating Function:**
$$M_X(t) = \left(\frac{1}{1 - \beta t}\right)^\alpha = (1 - \beta t)^{-\alpha}, \quad t < \frac{1}{\beta}$$

**Cumulant Generating Function:**
$$K_X(t) = \log M_X(t) = -\alpha \ln(1 - \beta t)$$

**Additivity (Same Scale):** If $X_i \sim \text{Gamma}(\alpha_i, \beta)$ independent then $\sum_i X_i \sim \text{Gamma}(\sum_i \alpha_i, \beta)$.

**Memoryless Note:** Only the special case $\alpha=1$ (Exponential) has the memoryless property.

**Special Cases:**
- **Exponential Distribution:** $\text{Gamma}(1, \beta) = \text{Exponential}(\beta)$ where $\beta$ is the scale (mean)
- **Chi-Square Distribution:** $\text{Gamma}(k/2, 2) = \chi^2(k)$

#### Example: Gamma Theory
Let's derive the MGF of the Gamma distribution and prove the reproductive property.

**Part 1: MGF Derivation**
For $X \sim \text{Gamma}(\alpha, \beta)$:

**Step 1:** Set up the integral
$$M_X(t) = E[e^{tX}] = \int_0^{\infty} e^{tx} \cdot \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta} dx$$

**Step 2:** Combine exponentials and rearrange
$$M_X(t) = \frac{1}{\beta^\alpha \Gamma(\alpha)} \int_0^{\infty} x^{\alpha-1} e^{-x/\beta + tx} dx = \frac{1}{\beta^\alpha \Gamma(\alpha)} \int_0^{\infty} x^{\alpha-1} e^{-x(1/\beta - t)} dx$$

**Step 3:** For convergence, we need $1/\beta - t > 0$ (i.e., $t < 1/\beta$)
Let $u = x(1/\beta - t)$, then $x = \frac{u}{1/\beta - t}$ and $dx = \frac{du}{1/\beta - t}$:

$$M_X(t) = \frac{1}{\beta^\alpha \Gamma(\alpha)} \int_0^{\infty} \left(\frac{u}{1/\beta - t}\right)^{\alpha-1} e^{-u} \frac{du}{1/\beta - t}$$

**Step 4:** Simplify
$$M_X(t) = \frac{1}{\beta^\alpha \Gamma(\alpha)} \cdot \frac{1}{(1/\beta - t)^\alpha} \int_0^{\infty} u^{\alpha-1} e^{-u} du$$

**Step 5:** Recognize the Gamma integral
$$\int_0^{\infty} u^{\alpha-1} e^{-u} du = \Gamma(\alpha)$$

Therefore: $$M_X(t) = \frac{1}{\beta^\alpha (1/\beta - t)^\alpha} = \left(\frac{1}{1 - \beta t}\right)^\alpha = (1 - \beta t)^{-\alpha}$$

**Part 2: Reproductive Property**
If $X_1 \sim \text{Gamma}(\alpha_1, \beta)$ and $X_2 \sim \text{Gamma}(\alpha_2, \beta)$ are independent, then $X_1 + X_2 \sim \text{Gamma}(\alpha_1 + \alpha_2, \beta)$.

**Proof:**
$$M_{X_1 + X_2}(t) = M_{X_1}(t) \cdot M_{X_2}(t) = (1 - \beta t)^{-\alpha_1} \cdot (1 - \beta t)^{-\alpha_2} = (1 - \beta t)^{-(\alpha_1 + \alpha_2)}$$

This is the MGF of $\text{Gamma}(\alpha_1 + \alpha_2, \beta)$. ✓

### 5.2 Beta Distribution

**Notation:** $X \sim \text{Beta}(\alpha, \beta)$

**Probability Density Function:**
$$f_X(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}, \quad 0 < x < 1$$

where $\alpha > 0$ and $\beta > 0$ are shape parameters.

**Beta Function:**
$$B(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)} = \int_0^1 t^{\alpha-1}(1-t)^{\beta-1} dt$$

**Expected Value and Variance:**
$$E[X] = \frac{\alpha}{\alpha + \beta}$$
$$\text{Var}(X) = \frac{\alpha\beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}$$

**Special Cases:**
- **Uniform Distribution:** $\text{Beta}(1, 1) = \text{Uniform}(0, 1)$
- **Symmetric about 1/2:** When $\alpha = \beta$

**Mode (when $\alpha, \beta > 1$):**
$$\text{Mode} = \frac{\alpha - 1}{\alpha + \beta - 2}$$

**Beta-Gamma Relationship:** If $Y_1 \sim \Gamma(\alpha, \theta)$ and $Y_2 \sim \Gamma(\beta, \theta)$ independent, then $\frac{Y_1}{Y_1 + Y_2} \sim \text{Beta}(\alpha, \beta)$.

**Order Statistic Connection:** $k$-th order statistic of i.i.d. $\text{Uniform}(0,1)$ is $\text{Beta}(k, n-k+1)$.

#### Example: Beta Properties
Let's prove the relationship between Beta and Gamma distributions and derive the order statistics result.

**Part 1: Beta-Gamma Relationship**
If $Y_1 \sim \text{Gamma}(\alpha, \theta)$ and $Y_2 \sim \text{Gamma}(\beta, \theta)$ are independent, then:
$$X = \frac{Y_1}{Y_1 + Y_2} \sim \text{Beta}(\alpha, \beta)$$

**Proof Sketch:**
**Step 1:** Let $U = Y_1 + Y_2$ and $V = \frac{Y_1}{Y_1 + Y_2}$

**Step 2:** The Jacobian of the transformation is:
$$J = \left|\frac{\partial(y_1, y_2)}{\partial(u, v)}\right| = u$$

**Step 3:** Since $Y_1 + Y_2 \sim \text{Gamma}(\alpha + \beta, \theta)$, using the transformation formula with Jacobian $|J| = u$, the joint density factorizes, and after simplification the marginal density of $V$ is:

**Step 4:** Simplifying and integrating out $u$:
$$f_V(v) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} v^{\alpha-1} (1-v)^{\beta-1}$$

This is the Beta$(\alpha, \beta)$ density. ✓

**Part 2: Order Statistics Result**
For $X_{(k)} = $ k-th smallest of $n$ i.i.d. Uniform$(0,1)$ random variables:

**Step 1:** The PDF of the k-th order statistic is:
$$f_{X_{(k)}}(x) = \frac{n!}{(k-1)!(n-k)!} [F(x)]^{k-1} [1-F(x)]^{n-k} f(x)$$

**Step 2:** For Uniform$(0,1)$: $F(x) = x$ and $f(x) = 1$, so:
$$f_{X_{(k)}}(x) = \frac{n!}{(k-1)!(n-k)!} x^{k-1} (1-x)^{n-k}, \quad 0 < x < 1$$

**Step 3:** Recognizing the Beta function:
$$f_{X_{(k)}}(x) = \frac{\Gamma(n+1)}{\Gamma(k)\Gamma(n-k+1)} x^{k-1} (1-x)^{n-k}$$

This is exactly Beta$(k, n-k+1)$. ✓

### 5.3 Chi-Square Distribution

**Notation:** $X \sim \chi^2(k)$ where $k$ is the degrees of freedom

**Probability Density Function:**
$$f_X(x) = \frac{1}{2^{k/2}\Gamma(k/2)} x^{k/2-1} e^{-x/2}, \quad x > 0$$

**Expected Value and Variance:**
$$E[X] = k$$
$$\text{Var}(X) = 2k$$

**Moment Generating Function:**
$$M_X(t) = (1 - 2t)^{-k/2}, \quad t < \frac{1}{2}$$

**Construction from Normal Distribution:**
If $Z_1, Z_2, \ldots, Z_k$ are independent $N(0,1)$ random variables, then:
$$\sum_{i=1}^k Z_i^2 \sim \chi^2(k)$$

**Reproductive Property:**
If $X_1 \sim \chi^2(k_1)$ and $X_2 \sim \chi^2(k_2)$ are independent, then:
$$X_1 + X_2 \sim \chi^2(k_1 + k_2)$$

**As Gamma:** $\chi^2(k) = \Gamma(k/2, 1/2)$ (shape $k/2$, rate $1/2$) – useful for MGFs and additivity.

### 5.4 Chebyshev's Theorem

**Statement:**
For any random variable $X$ with finite mean $\mu$ and variance $\sigma^2$, and for any $k > 1$:
$$P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}$$

**Equivalent Form:**
$$P(|X - \mu| < k\sigma) \geq 1 - \frac{1}{k^2}$$

**Mathematical Proof:**
Let $Y = (X - \mu)^2$. By Markov's inequality:
$$P(Y \geq (k\sigma)^2) \leq \frac{E[Y]}{(k\sigma)^2} = \frac{\sigma^2}{k^2\sigma^2} = \frac{1}{k^2}$$

Since $\{|X - \mu| \geq k\sigma\} = \{(X - \mu)^2 \geq k^2\sigma^2\}$, the result follows.

**Applications:**
- Provides distribution-free bounds on tail probabilities
- Useful when the exact distribution is unknown
- Foundation for the Weak Law of Large Numbers

**Example Applications:**
- For $k = 2$: At least 75% of data falls within 2 standard deviations
- For $k = 3$: At least 89% of data falls within 3 standard deviations

#### Example: Stock Price Analysis
A stock has an expected daily return of $\mu = 0.05\%$ and standard deviation $\sigma = 2\%$. Use Chebyshev's theorem to bound the probability that the daily return is between -3.95% and 4.05%.

**Solution:**
We want $P(|\text{Return} - 0.05| \leq 4)$, where the deviation is $4\% = 2 \times 2\% = 2\sigma$.

Using Chebyshev's theorem with $k = 2$:
$$P(|\text{Return} - 0.05| \leq 2\sigma) \geq 1 - \frac{1}{k^2} = 1 - \frac{1}{4} = 0.75$$

So at least 75% of daily returns fall between -3.95% and 4.05%.

**Comparison with Normal Distribution:** If returns were normally distributed, we'd get approximately 95% within 2 standard deviations (much tighter than Chebyshev's bound).

### 5.5 Markov's Inequality

**Statement:**
For any non-negative random variable $X$ and any $a > 0$:
$$P(X \geq a) \leq \frac{E[X]}{a}$$

**Mathematical Proof:**
$$E[X] = \int_0^{\infty} x f_X(x) dx \geq \int_a^{\infty} x f_X(x) dx \geq a \int_a^{\infty} f_X(x) dx = a \cdot P(X \geq a)$$

**Generalized Form:**
For any random variable $X$ and any non-decreasing function $g$ with $g(x) \geq 0$:
$$P(X \geq a) \leq \frac{E[g(X)]}{g(a)}$$

**Connection to Chebyshev's Theorem:**
Chebyshev's inequality is a special case of Markov's inequality applied to $Y = (X - \mu)^2$.

**Applications:**
- Tail bound estimation
- Convergence analysis
- Concentration inequalities

#### Example: Website Loading Times
A website's loading time has an expected value of 3 seconds. What can we say about the probability that a page takes more than 10 seconds to load?

Using Markov's inequality with $E[X] = 3$ and $a = 10$:
$$P(X \geq 10) \leq \frac{E[X]}{10} = \frac{3}{10} = 0.3$$

So at most 30% of page loads take 10 seconds or longer.

**More refined analysis:** If we also know the loading time is never negative and has variance $\sigma^2 = 4$, we can use both inequalities:
- Markov: $P(X \geq 10) \leq 0.3$
- Chebyshev (for $P(X \geq 10)$ where $10 = 3 + 7 = \mu + 3.5\sigma$):
  $$P(X \geq 10) \leq P(|X - 3| \geq 7) \leq \frac{4}{49} \approx 0.082$$

Chebyshev gives a tighter bound when we have variance information.

### 5.6 Comparison Summary Table

| Distribution | Parameters (rate form) | Mean | Variance | MGF Domain | Notes |
|-------------|------------------------|------|----------|-----------|-------|
| Exponential | $\beta$ | $\beta$ | $\beta^2$ | $t<1/\beta$ | Memoryless (only case) |
| Gamma | $\alpha,\beta$ | $\alpha\beta$ | $\alpha\beta^2$ | $t<1/\beta$ | Sum-stable (same scale) |
| Chi-Square | $k$ | $k$ | $2k$ | $t<1/2$ | Gamma special case |
| Beta | $\alpha,\beta$ | $\alpha/(\alpha+\beta)$ | $\frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}$ | — | Support (0,1) |

**Key Themes:** Reproductive properties (Gamma / Chi-Square), conjugacy (Beta-Binomial), universal bounds (Markov / Chebyshev) vs distribution‑specific sharper results.

---

## Week 6: Joint, Marginal, and Conditional Distributions

**Reading:** Wackerly et al. Sections 5.1-5.4 (32 pp.)

**Learning Objectives:**
- Calculate probabilities of events using joint distributions
- Derive marginal and conditional distributions

[Back to Table of Contents](#table-of-contents)

### 6.1 Joint Probability Distributions (Sections 5.1-5.2)

**Discrete Case:**
For discrete random variables $X$ and $Y$, the joint probability mass function is:
$$p_{X,Y}(x,y) = P(X = x, Y = y)$$

**Properties:**
- $p_{X,Y}(x,y) \geq 0$ for all $(x,y)$
- $\sum_x \sum_y p_{X,Y}(x,y) = 1$

**Continuous Case:**
For continuous random variables $X$ and $Y$, the joint probability density function satisfies:
$$f_{X,Y}(x,y) \geq 0 \quad \text{and} \quad \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dx \, dy = 1$$

**Joint Cumulative Distribution Function:**
$$F_{X,Y}(x,y) = P(X \leq x, Y \leq y)$$

**Relationship between CDF and PDF:**
$$f_{X,Y}(x,y) = \frac{\partial^2 F_{X,Y}(x,y)}{\partial x \partial y}$$

**Probability Calculations:**
$$P(a < X \leq b, c < Y \leq d) = \int_c^d \int_a^b f_{X,Y}(x,y) \, dx \, dy$$

### 6.2 Marginal Distributions

**Discrete Case:**
$$p_X(x) = \sum_y p_{X,Y}(x,y)$$
$$p_Y(y) = \sum_x p_{X,Y}(x,y)$$

**Continuous Case:**
$$f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dy$$
$$f_Y(y) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dx$$

**Marginal CDFs:**
$$F_X(x) = \lim_{y \to \infty} F_{X,Y}(x,y)$$
$$F_Y(y) = \lim_{x \to \infty} F_{X,Y}(x,y)$$

**Expected Values:**
$$E[X] = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x \cdot f_{X,Y}(x,y) \, dx \, dy = \int_{-\infty}^{\infty} x \cdot f_X(x) \, dx$$

### 6.3 Conditional Distributions

**Discrete Case:**
$$p_{Y|X}(y|x) = \frac{p_{X,Y}(x,y)}{p_X(x)}, \quad \text{provided } p_X(x) > 0$$

**Continuous Case:**
$$f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}, \quad \text{provided } f_X(x) > 0$$

**Conditional Expectation:**
$$E[Y|X = x] = \int_{-\infty}^{\infty} y \cdot f_{Y|X}(y|x) \, dy$$

**Law of Total Expectation:**
$$E[Y] = E[E[Y|X]] = \int_{-\infty}^{\infty} E[Y|X = x] \cdot f_X(x) \, dx$$

**Conditional Variance:**
$$\text{Var}(Y|X = x) = E[Y^2|X = x] - (E[Y|X = x])^2$$

**Law of Total Variance:**
$$\text{Var}(Y) = E[\text{Var}(Y|X)] + \text{Var}(E[Y|X])$$

### 6.4 Independence of Random Variables

**Definition:**
Random variables $X$ and $Y$ are independent if and only if:
$$F_{X,Y}(x,y) = F_X(x) \cdot F_Y(y) \quad \text{for all } x, y$$

**Equivalent Conditions:**

**For Discrete Variables:**
$$p_{X,Y}(x,y) = p_X(x) \cdot p_Y(y) \quad \text{for all } x, y$$

**For Continuous Variables:**
$$f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y) \quad \text{for all } x, y$$

**Conditional Independence:**
$$f_{Y|X}(y|x) = f_Y(y) \quad \text{and} \quad f_{X|Y}(x|y) = f_X(x)$$

**Properties of Independent Variables:**
- $E[XY] = E[X] \cdot E[Y]$
- $\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y)$
- $M_{X+Y}(t) = M_X(t) \cdot M_Y(t)$
- $\text{Cov}(X,Y) = 0$ (but zero covariance doesn't imply independence)

### 6.5 Transformations of Bivariate Distributions

**Linear Transformations:**
If $(X,Y)$ has joint density $f_{X,Y}(x,y)$ and we define:
$$U = aX + bY + c$$
$$V = dX + eY + f$$

where $J = |ae - bd| \neq 0$ (the Jacobian), then:
$$f_{U,V}(u,v) = \frac{1}{|J|} f_{X,Y}(x(u,v), y(u,v))$$

**General Transformation Method:**
For transformations $U = g_1(X,Y)$ and $V = g_2(X,Y)$:

1. Find the inverse transformation: $X = h_1(U,V)$, $Y = h_2(U,V)$
2. Compute the Jacobian: $J = \left|\frac{\partial(x,y)}{\partial(u,v)}\right|$
3. Apply the transformation formula: $f_{U,V}(u,v) = f_{X,Y}(h_1(u,v), h_2(u,v)) \cdot |J|$

**Sum and Difference:**
For independent $X$ and $Y$:
- $U = X + Y$: $f_U(u) = \int_{-\infty}^{\infty} f_X(x) f_Y(u-x) dx$ (convolution)
- $V = X - Y$: $f_V(v) = \int_{-\infty}^{\infty} f_X(x) f_Y(x-v) dx$

**Bivariate Normal Distribution:**
$$(X,Y) \sim BN(\mu_1, \mu_2, \sigma_1^2, \sigma_2^2, \rho)$$

Joint PDF:
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} \exp\left(-\frac{Q}{2(1-\rho^2)}\right)$$

where:
$$Q = \frac{(x-\mu_1)^2}{\sigma_1^2} - \frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2} + \frac{(y-\mu_2)^2}{\sigma_2^2}$$

**Properties:**
- Marginal distributions: $X \sim N(\mu_1, \sigma_1^2)$, $Y \sim N(\mu_2, \sigma_2^2)$
- Independence: $X$ and $Y$ are independent if and only if $\rho = 0$
- Linear combinations are normal: $aX + bY \sim N(a\mu_1 + b\mu_2, a^2\sigma_1^2 + b^2\sigma_2^2 + 2ab\rho\sigma_1\sigma_2)$

#### Example: Joint Distribution Analysis
Consider the joint distribution of height (X) and weight (Y) with:
- $X \sim N(70, 16)$ (height in inches, $\sigma_X = 4$)
- $Y \sim N(150, 400)$ (weight in pounds, $\sigma_Y = 20$)
- $\rho = 0.6$ (positive correlation)

**Question 1:** Find $P(X > 72, Y > 160)$
This requires numerical integration of the bivariate normal density over the region $\{(x,y): x > 72, y > 160\}$.

**Question 2:** Find the conditional distribution of weight given height = 74 inches.
For bivariate normal, $Y|X=x \sim N(\mu_Y + \rho\frac{\sigma_Y}{\sigma_X}(x-\mu_X), \sigma_Y^2(1-\rho^2))$

$$Y|X=74 \sim N\left(150 + 0.6 \times \frac{20}{4}(74-70), 400(1-0.6^2)\right)$$
$$= N(150 + 0.6 \times 5 \times 4, 400 \times 0.64)$$
$$= N(162, 256)$$

**Question 3:** Are height and weight independent?
No, because $\rho = 0.6 \neq 0$.

**Question 4:** Find the distribution of total "health index" $Z = 2X + Y$
$$Z \sim N(2 \times 70 + 150, 4 \times 16 + 400 + 2 \times 2 \times 1 \times 0.6 \times 4 \times 20)$$
$$= N(290, 64 + 400 + 960) = N(290, 1424)$$

So $Z \sim N(290, 1424)$ with standard deviation $\sqrt{1424} \approx 37.7$.

**Derivation: General Transformation Formula and Convolution**

Let's derive the general transformation formula for bivariate distributions and prove the convolution formula.

**Part 1: General Transformation Formula**
Given $(X, Y)$ with joint PDF $f_{X,Y}(x, y)$ and transformations:
$$U = g_1(X, Y), \quad V = g_2(X, Y)$$

**Step 1:** Find the inverse transformation
$$X = h_1(U, V), \quad Y = h_2(U, V)$$

**Step 2:** Compute the Jacobian
$$J = \left|\frac{\partial(x, y)}{\partial(u, v)}\right| = \left|\begin{matrix}
\frac{\partial h_1}{\partial u} & \frac{\partial h_1}{\partial v} \\
\frac{\partial h_2}{\partial u} & \frac{\partial h_2}{\partial v}
\end{matrix}\right|$$

**Step 3:** Apply the transformation formula
$$f_{U,V}(u, v) = f_{X,Y}(h_1(u, v), h_2(u, v)) \cdot |J|$$

**Part 2: Convolution Formula Derivation**
For independent $X$ and $Y$, derive the PDF of $U = X + Y$.

**Method 1: Direct Integration**
**Step 1:** Use the definition of CDF
$$F_U(u) = P(X + Y \leq u) = \int\int_{x+y \leq u} f_X(x) f_Y(y) \, dx \, dy$$

**Step 2:** Change the order of integration
$$F_U(u) = \int_{-\infty}^{\infty} f_X(x) \left[\int_{-\infty}^{u-x} f_Y(y) \, dy\right] dx = \int_{-\infty}^{\infty} f_X(x) F_Y(u-x) \, dx$$

**Step 3:** Differentiate to get PDF
$$f_U(u) = \frac{d}{du} F_U(u) = \int_{-\infty}^{\infty} f_X(x) f_Y(u-x) \, dx$$

**Method 2: Using Transformation**
**Step 1:** Set $U = X + Y, V = X$
Inverse: $X = V, Y = U - V$

**Step 2:** Jacobian is
$$J = \left|\begin{matrix} 0 & 1 \\ 1 & -1 \end{matrix}\right| = -1$$

**Step 3:** Apply transformation
$$f_{U,V}(u, v) = f_{X,Y}(v, u-v) \cdot 1 = f_X(v) f_Y(u-v)$$

**Step 4:** Marginalize over $V$
$$f_U(u) = \int_{-\infty}^{\infty} f_X(v) f_Y(u-v) \, dv$$

Both methods yield the convolution formula. ✓

**Part 3: MGF Verification**
For independent $X, Y$:
$$M_{X+Y}(t) = E[e^{t(X+Y)}] = E[e^{tX}e^{tY}] = E[e^{tX}]E[e^{tY}] = M_X(t)M_Y(t)$$

This confirms that MGFs multiply for sums of independent random variables. ✓

---

## Week 7: Means, Variances, and Covariances for Bivariate Distributions

**Reading:** Wackerly et al. Sections 5.5-5.9, 5.11 (29 pp.)

**Learning Objectives:**
- Calculate expected value of functions of random variables
- Calculate expectations of sums and products of random variables
- Calculate means, variances, and covariances using joint distributions
- Calculate conditional expectations

[Back to Table of Contents](#table-of-contents)

### 7.1 Expected Values for Functions of Random Variables (Section 5.5)
Law of the Unconscious Statistician (LOTUS): For continuous \(X\), \(E[g(X)]=\int g(x)f_X(x)dx\); for joint: \(E[h(X,Y)]=\iint h(x,y)f_{X,Y}(x,y)dxdy\).

### 7.2 Linearity and Sums of RVs
Linearity holds without independence: \(E\big[\sum a_i X_i\big]=\sum a_i E[X_i]\). If independent, MGF of sum is product; variances add.

### 7.3 Covariance and Correlation
\(\operatorname{Cov}(X,Y)=E[XY]-E[X]E[Y]\); correlation \(\rho=\frac{\operatorname{Cov}(X,Y)}{\sigma_X\sigma_Y}\). Uncorrelated does not imply independent (except special families e.g. multivariate normal).

### 7.4 Variance of Linear Combinations
For vector $\mathbf{X}$ with covariance matrix $\Sigma$ and constants $\mathbf{a}$: $\operatorname{Var}(\mathbf{a}^T\mathbf{X}) = \mathbf{a}^T \Sigma \, \mathbf{a}$. For two variables: $\operatorname{Var}(aX + bY) = a^2 \operatorname{Var}(X) + b^2 \operatorname{Var}(Y) + 2ab \, \operatorname{Cov}(X,Y)$.

### 7.5 Laws of Total Expectation and Variance
$E[Y] = E[E[Y|X]]$; $\operatorname{Var}(Y) = E[\operatorname{Var}(Y|X)] + \operatorname{Var}(E[Y|X])$. Useful for hierarchical decompositions.

### 7.6 Sample Mean Properties
For i.i.d. $X_i$ with mean $\mu$ and variance $\sigma^2$: $\bar{X} = n^{-1}\sum X_i$ has $E[\bar{X}] = \mu$, $\operatorname{Var}(\bar{X}) = \sigma^2 / n$ (variance reduction by averaging).

### 7.7 Bivariate Normal and Conditional Results
State joint density, conditional mean/variance formulas, and independence iff zero covariance.

### 7.8 Applications (Portfolio, Aggregation)
Portfolio variance minimization, risk pooling, and sum-of-Poisson / reproductive properties as variance reduction illustrations.

### 7.9 Applications and Examples

#### Example: Bivariate Normal Distribution

The bivariate normal distribution is a fundamental joint distribution:

**Joint PDF:**
$$f_{X,Y}(x,y) = \frac{1}{2\pi\sigma_X\sigma_Y\sqrt{1-\rho^2}} \exp\left[-\frac{1}{2(1-\rho^2)}\left(\frac{(x-\mu_X)^2}{\sigma_X^2} - \frac{2\rho(x-\mu_X)(y-\mu_Y)}{\sigma_X\sigma_Y} + \frac{(y-\mu_Y)^2}{\sigma_Y^2}\right)\right]$$

where $\rho$ is the correlation coefficient.

**Properties:**
- Marginals: $X \sim N(\mu_X, \sigma_X^2)$ and $Y \sim N(\mu_Y, \sigma_Y^2)$
- $E[X] = \mu_X$, $E[Y] = \mu_Y$
- $\text{Cov}(X,Y) = \rho\sigma_X\sigma_Y$
- If $\rho = 0$, then $X$ and $Y$ are independent (special property of normal distribution)

**Conditional Distribution:**
$$Y|X=x \sim N\left(\mu_Y + \rho\frac{\sigma_Y}{\sigma_X}(x-\mu_X), \sigma_Y^2(1-\rho^2)\right)$$

This is the foundation for linear regression!

#### Example: Sum of Random Variables

Given independent random variables $X_1, X_2, \ldots, X_n$ with $E[X_i] = \mu$ and $\text{Var}(X_i) = \sigma^2$.

Let $\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i$

**Expected value of sample mean:**
$$E[\bar{X}] = E\left[\frac{1}{n}\sum_{i=1}^{n} X_i\right] = \frac{1}{n}\sum_{i=1}^{n} E[X_i] = \frac{1}{n} \cdot n\mu = \mu$$

**Variance of sample mean:**
$$\text{Var}(\bar{X}) = \text{Var}\left(\frac{1}{n}\sum_{i=1}^{n} X_i\right) = \frac{1}{n^2}\sum_{i=1}^{n} \text{Var}(X_i) = \frac{1}{n^2} \cdot n\sigma^2 = \frac{\sigma^2}{n}$$

This shows that the sample mean has the same expected value as each observation but with reduced variance, which is fundamental to statistical inference.

#### Example: Portfolio Variance

In finance, consider a portfolio of two assets with returns $X$ and $Y$. Invest fraction $w$ in asset $X$ and $(1-w)$ in asset $Y$.

**Portfolio return:**
$$R = wX + (1-w)Y$$

**Expected return:**
$$E[R] = wE[X] + (1-w)E[Y]$$

**Portfolio variance:**
$$\text{Var}(R) = w^2\text{Var}(X) + (1-w)^2\text{Var}(Y) + 2w(1-w)\text{Cov}(X,Y)$$

**Optimal weight to minimize variance:**
Taking derivative with respect to $w$ and setting to zero:
$$w^* = \frac{\text{Var}(Y) - \text{Cov}(X,Y)}{\text{Var}(X) + \text{Var}(Y) - 2\text{Cov}(X,Y)}$$

This demonstrates why diversification reduces risk when assets are not perfectly correlated.

**Quick Jump:** [Week 7](#week-7-means-variances-and-covariances) | [Summary](#summary-and-key-points)

---

## Week 8: Midterm Exam

[Back to Table of Contents](#table-of-contents)

### 8.1 Midterm Structure and Focus
The midterm (Week 8) assesses concepts from Weeks 1–7:
- Core probability axioms, conditional probability, Bayes' theorem
- Discrete distributions: Binomial, Geometric, Hypergeometric, Poisson
- Continuous & special distributions: Uniform, Normal, Gamma, Beta, Chi-Square
- Inequalities: Markov, Chebyshev
- Moment generating functions and moment calculations
- Joint, marginal, conditional distributions; independence criteria
- Expectation of functions (LOTUS), covariance, variance of linear combinations
- Laws of total expectation and total variance; sample mean properties

### 8.2 Recommended Formula Sheet Inclusions
Your allowed formula sheet should concisely include:
- PMFs/PDFs and CDF forms (no derivations)
- MGFs for Binomial, Poisson, Normal, Gamma, Chi-Square
- Mean/variance summaries for all core distributions
- Transformation Jacobian template
- Variance of linear combination and covariance definition
- Law of total expectation / variance identities
- Chebyshev and Markov forms

### 8.3 Strategic Review Checklist
1. Can you derive Binomial MGF and use it to get mean/variance quickly?
2. Can you recognize when to use Gamma additivity or Beta-Gamma relationships?
3. Can you set up and compute a marginal from a joint density (integration bounds)?
4. Do you fluently apply LOTUS to $E[g(X,Y)]$ without re-deriving distributions?
5. Can you distinguish when independence is NOT implied by zero covariance?

### 8.4 Common Pitfalls
- Forgetting support when writing densities (e.g., $x>0$, $0<x<1$)
- Dropping the Jacobian absolute value in transformations
- Misapplying Chebyshev as an equality rather than an upper bound
- Confusing $\text{Var}(aX + b)$ with $a^2\text{Var}(X)$ (b does not contribute)
- Treating uncorrelated non-normal pairs as independent

### 8.5 Rapid Practice Set (Self-Check)
1. Derive $E[X]$ and $\text{Var}(X)$ for $X\sim$ Hypergeometric using indicator sums.
2. Show $\sum_{i=1}^n Z_i^2 \sim \chi^2(n)$ via MGF (normal square property).
3. Given joint density $f_{X,Y}(x,y)=2$ on $0<y<x<1$, find $f_X(x)$ and $E[Y|X=x]$.
4. Prove LOTUS for a discrete non-negative integer variable using series expansion.
5. For independent $X,Y$ with same variance, find $w$ minimizing $\text{Var}(wX+(1-w)Y)$.

### 8.6 Suggested Final 48-Hour Plan
- T-48h: Redo 5 mixed distribution problems (at least one transformation)
- T-36h: Summarize each distribution (name → support → parameters → mean/var → MGF)
- T-24h: Drill conditioning & law of total variance mini proofs
- T-12h: Work two timed past-style problems (simulate exam pacing)
- T-2h: Light review of formula sheet only—no new problem types

### 8.7 Mental Models Recap
- "MGF as a moment factory" → differentiate, evaluate at 0
- "Support first" → write domain before manipulating a density
- "Variance splits" → structural + conditional via law of total variance
- "Additivity triggers" → Gamma shape adds when rate matches; Normal sums stay normal

**Quick Jump:** [Week 7](#week-7-means-variances-and-covariances) | [Week 9](#week-9-functions-of-random-variables) | [Week 10](#week-10-sampling-distributions-and-limit-theorems) | [Summary](#summary-and-key-points)

---

## Week 9: Functions of Random Variables

**Reading:** Wackerly et al. Sections 6.1-6.5, 6.7 (37 pp.)

**Learning Objectives:**
- Calculate distributions of functions of random variables

**Note:** Section 6.6 (Multivariable Transformations Using Jacobians) is optional and may be skipped.

[Back to Table of Contents](#table-of-contents)

### 9.1 Motivation and Overview (Section 6.1)
We move from studying individual and jointly distributed variables to the distribution of transformed quantities: $Y = g(X)$ or $(U,V) = g(X,Y)$. Core tasks:
- Derive CDF / PDF of a function of one variable
- Use change of variables (Jacobian) for multivariate transformations
- Obtain distributions of sums (convolution), products, ratios
- Characterize order statistics (min, max, general $k$th)
- Provide groundwork for sampling distributions (next week) and delta method variance propagation

### 9.2 Method of Distribution (One-to-One on Support Pieces)

**Basic Approach:** For $Y = g(X)$ with $g$ monotone on support of $X$:

**Step 1 - Find CDF:**
$$F_Y(y) = P(Y \le y) = P(g(X) \le y)$$

**Step 2 - Convert to X:**
- If $g$ is strictly increasing: $P(g(X) \le y) = P(X \le g^{-1}(y)) = F_X(g^{-1}(y))$
- If $g$ is strictly decreasing: $P(g(X) \le y) = P(X \ge g^{-1}(y)) = 1 - F_X(g^{-1}(y))$

**Step 3 - Differentiate to get PDF:**
$$f_Y(y) = f_X(g^{-1}(y))\left|\frac{d}{dy}g^{-1}(y)\right|$$

The absolute value handles both increasing and decreasing cases.

**Alternative Formula (using $x = g^{-1}(y)$):**
$$f_Y(y) = f_X(x) \left|\frac{dx}{dy}\right|$$

#### Example 9.2A: Exponential Scaling

**Problem:** If $X \sim \text{Exp}(\lambda)$ with PDF $f_X(x) = \lambda e^{-\lambda x}$ for $x > 0$, and $Y = cX$ where $c > 0$, find the distribution of $Y$.

**Solution:**

**Step 1:** Identify the transformation
- $Y = cX$, so $X = Y/c = g^{-1}(Y)$
- $\frac{dx}{dy} = \frac{1}{c}$

**Step 2:** Apply the transformation formula
$$f_Y(y) = f_X(y/c) \cdot \left|\frac{1}{c}\right| = \frac{1}{c} \cdot \lambda e^{-\lambda(y/c)} = \frac{\lambda}{c} e^{-(\lambda/c)y}$$

**Step 3:** Identify the distribution
$$f_Y(y) = \frac{\lambda}{c} e^{-(\lambda/c)y}, \quad y > 0$$

This is $\text{Exp}(\lambda/c)$ — the rate rescales inversely with the scaling constant!

**Interpretation:**
- If you measure time in hours ($X$) and convert to minutes ($Y = 60X$), the rate parameter becomes $\lambda/60$.
- If events occur at rate $\lambda = 2$ per hour, they occur at rate $2/60 = 1/30$ per minute.

#### Example 9.2B: Logarithmic Transformation

**Problem:** If $X \sim \text{Uniform}(0, 1)$ and $Y = -\log(X)$, find the distribution of $Y$.

**Solution:**

**Step 1:** Find the CDF of $Y$
$$F_Y(y) = P(Y \le y) = P(-\log(X) \le y) = P(\log(X) \ge -y) = P(X \ge e^{-y})$$

For $X \sim \text{Uniform}(0,1)$: $F_X(x) = x$, so:
$$F_Y(y) = 1 - F_X(e^{-y}) = 1 - e^{-y}, \quad y > 0$$

**Step 2:** Differentiate to get PDF
$$f_Y(y) = \frac{d}{dy}(1 - e^{-y}) = e^{-y}, \quad y > 0$$

**Conclusion:** $Y \sim \text{Exp}(1)$ — This is a fundamental result for random number generation!

### 9.3 Many-to-One Transformations (Partition Method)

**Problem:** When $g$ is not one-to-one (injective), multiple values of $X$ can produce the same $Y$.

**Method:**
1. For each $y$, solve $g(x) = y$ to find all roots $x_1(y), x_2(y), \ldots, x_k(y)$ in the support of $X$
2. Apply the formula:
$$f_Y(y) = \sum_{i=1}^{k} f_X(x_i(y)) \left| \frac{dx_i(y)}{dy} \right|$$

**Key Insight:** Each "branch" of the inverse contributes to the density of $Y$.

#### Example 9.3A: Squared Standard Normal

**Problem:** If $X \sim N(0,1)$ and $Y = X^2$, find the distribution of $Y$.

**Solution:**

**Step 1:** Find all roots of $x^2 = y$
- For $y > 0$: $x_1(y) = \sqrt{y}$ and $x_2(y) = -\sqrt{y}$
- For $y \leq 0$: No solutions (not in the support of $Y$)

**Step 2:** Calculate derivatives
$$\frac{dx_1}{dy} = \frac{1}{2\sqrt{y}}, \quad \frac{dx_2}{dy} = -\frac{1}{2\sqrt{y}}$$

**Step 3:** Apply the partition formula
$$f_Y(y) = f_X(\sqrt{y})\left|\frac{1}{2\sqrt{y}}\right| + f_X(-\sqrt{y})\left|-\frac{1}{2\sqrt{y}}\right|$$

**Step 4:** Substitute the standard normal PDF $f_X(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}$

Since $f_X(\sqrt{y}) = f_X(-\sqrt{y})$ (by symmetry):
$$f_Y(y) = 2 \cdot \frac{1}{\sqrt{2\pi}}e^{-y/2} \cdot \frac{1}{2\sqrt{y}} = \frac{1}{\sqrt{2\pi y}} e^{-y/2}, \quad y > 0$$

**Step 5:** Recognize the distribution
$$f_Y(y) = \frac{1}{2^{1/2}\Gamma(1/2)} y^{1/2-1} e^{-y/2} = \frac{1}{\sqrt{2\pi y}} e^{-y/2}$$

This is the $\chi^2(1)$ distribution, which is $\text{Gamma}(1/2, 1/2)$.

**Important Result:** If $Z \sim N(0,1)$, then $Z^2 \sim \chi^2(1)$.

#### Example 9.3B: Absolute Value Transformation

**Problem:** If $X \sim N(0, \sigma^2)$ and $Y = |X|$, find $f_Y(y)$.

**Solution:**

**Step 1:** For $y > 0$, solve $|x| = y$
- $x_1(y) = y$ and $x_2(y) = -y$

**Step 2:** Calculate
$$\frac{dx_1}{dy} = 1, \quad \frac{dx_2}{dy} = -1$$

**Step 3:** Apply formula
$$f_Y(y) = f_X(y) \cdot 1 + f_X(-y) \cdot 1 = 2f_X(y) = \frac{2}{\sigma\sqrt{2\pi}}e^{-y^2/(2\sigma^2)}, \quad y > 0$$

This is called the **half-normal distribution**.

### 9.4 Change of Variables (Multivariate Jacobian Method)

**Theorem:** Let $(X,Y)$ have joint density $f_{X,Y}(x,y)$. Define a transformation:
$$(U,V) = g(X,Y)$$

with inverse $(X,Y) = h(U,V)$, where $h$ is one-to-one and differentiable. Then:
$$f_{U,V}(u,v) = f_{X,Y}(h_1(u,v), h_2(u,v)) \left| J(u,v) \right|$$

where $J$ is the **Jacobian determinant** of the inverse transformation:
$$J = \det \begin{pmatrix} \frac{\partial x}{\partial u} & \frac{\partial x}{\partial v} \\ \frac{\partial y}{\partial u} & \frac{\partial y}{\partial v} \end{pmatrix} = \frac{\partial x}{\partial u}\frac{\partial y}{\partial v} - \frac{\partial x}{\partial v}\frac{\partial y}{\partial u}$$

**Important:** The Jacobian uses the **inverse** transformation $(x,y) = h(u,v)$.

#### Example 9.4A: Sum and Difference of Independent Variables

**Problem:** Let $X$ and $Y$ be independent continuous random variables. Define:
$$U = X + Y, \quad V = X - Y$$

Find the joint distribution of $(U,V)$.

**Solution:**

**Step 1:** Find the inverse transformation
From $U = X + Y$ and $V = X - Y$:
$$X = \frac{U + V}{2}, \quad Y = \frac{U - V}{2}$$

**Step 2:** Calculate the Jacobian
$$\frac{\partial x}{\partial u} = \frac{1}{2}, \quad \frac{\partial x}{\partial v} = \frac{1}{2}$$
$$\frac{\partial y}{\partial u} = \frac{1}{2}, \quad \frac{\partial y}{\partial v} = -\frac{1}{2}$$

$$J = \det \begin{pmatrix} 1/2 & 1/2 \\ 1/2 & -1/2 \end{pmatrix} = \frac{1}{2} \cdot \left(-\frac{1}{2}\right) - \frac{1}{2} \cdot \frac{1}{2} = -\frac{1}{4} - \frac{1}{4} = -\frac{1}{2}$$

$$|J| = \frac{1}{2}$$

**Step 3:** Apply the transformation formula
$$f_{U,V}(u,v) = f_{X,Y}\left(\frac{u+v}{2}, \frac{u-v}{2}\right) \cdot \frac{1}{2}$$

If $X$ and $Y$ are independent:
$$f_{U,V}(u,v) = f_X\left(\frac{u+v}{2}\right) f_Y\left(\frac{u-v}{2}\right) \cdot \frac{1}{2}$$

#### Example 9.4B: Box-Muller Transformation

**Problem:** The Box-Muller method generates two independent standard normals from two independent uniform random variables. If $U_1, U_2 \sim \text{Uniform}(0,1)$ independent, define:
$$X = \sqrt{-2\ln U_1} \cos(2\pi U_2)$$
$$Y = \sqrt{-2\ln U_1} \sin(2\pi U_2)$$

Show that $X$ and $Y$ are independent $N(0,1)$.

**Solution Sketch:**

**Step 1:** Use polar coordinates
Let $R^2 = X^2 + Y^2 = -2\ln U_1$ and $\Theta = 2\pi U_2$.

**Step 2:** Note that $R^2 \sim \text{Exp}(1/2) = \chi^2(2)$ (from $U_1 \sim \text{Uniform}(0,1)$)

**Step 3:** $\Theta \sim \text{Uniform}(0, 2\pi)$, independent of $R$

**Step 4:** The Jacobian transformation from $(R, \Theta)$ to $(X, Y)$ with $X = R\cos\Theta$, $Y = R\sin\Theta$ gives:
$$f_{X,Y}(x,y) = \frac{1}{2\pi}e^{-(x^2+y^2)/2} = \frac{1}{\sqrt{2\pi}}e^{-x^2/2} \cdot \frac{1}{\sqrt{2\pi}}e^{-y^2/2}$$

This shows $X, Y \overset{iid}{\sim} N(0,1)$!

### 9.5 Convolutions (Sum of Independent Variables)

**Continuous Case:** For independent continuous random variables $X$ and $Y$, the PDF of $S = X + Y$ is:
$$f_S(s) = \int_{-\infty}^{\infty} f_X(x) f_Y(s-x) \, dx = \int_{-\infty}^{\infty} f_X(s-y) f_Y(y) \, dy$$

This is called the **convolution** of $f_X$ and $f_Y$, denoted $f_X * f_Y$.

**Discrete Case:** For discrete random variables:
$$P(S = s) = \sum_x P(X = x) P(Y = s - x)$$

**Key Properties:**
- Convolution is commutative: $f_X * f_Y = f_Y * f_X$
- Sums of independent normals are normal
- Sums of independent Poissons are Poisson
- Sums of independent Gammas (same rate) are Gamma

#### Example 9.5A: Sum of Independent Exponentials

**Problem:** If $X_1, X_2 \sim \text{Exp}(\lambda)$ are independent, find the distribution of $S = X_1 + X_2$.

**Solution:**

**Method 1: Convolution**

The PDF of $X_i$ is $f(x) = \lambda e^{-\lambda x}$ for $x > 0$.

$$f_S(s) = \int_{-\infty}^{\infty} f_{X_1}(x) f_{X_2}(s-x) \, dx$$

Since both are exponential with support on $(0, \infty)$, we need $0 < x < s$:
$$f_S(s) = \int_0^s \lambda e^{-\lambda x} \cdot \lambda e^{-\lambda(s-x)} \, dx$$

$$= \lambda^2 e^{-\lambda s} \int_0^s dx = \lambda^2 s e^{-\lambda s}, \quad s > 0$$

**Method 2: Recognize the pattern**
$$f_S(s) = \lambda^2 s e^{-\lambda s} = \frac{\lambda^2}{\Gamma(2)} s^{2-1} e^{-\lambda s}$$

This is $\text{Gamma}(2, \lambda)$ or equivalently $\text{Erlang}(2, \lambda)$.

**Generalization:** If $X_1, \ldots, X_n \overset{iid}{\sim} \text{Exp}(\lambda)$, then:
$$S_n = \sum_{i=1}^n X_i \sim \text{Gamma}(n, \lambda)$$

#### Example 9.5B: Sum of Independent Uniform Variables

**Problem:** If $X_1, X_2 \sim \text{Uniform}(0,1)$ independent, find the distribution of $S = X_1 + X_2$.

**Solution:**

The PDF of each $X_i$ is $f(x) = 1$ for $0 < x < 1$.

$$f_S(s) = \int_{-\infty}^{\infty} f_{X_1}(x) f_{X_2}(s-x) \, dx$$

We need both $0 < x < 1$ and $0 < s - x < 1$, i.e., $s-1 < x < s$.

**Case 1:** $0 < s \leq 1$
The overlap is $0 < x < s$:
$$f_S(s) = \int_0^s 1 \cdot 1 \, dx = s$$

**Case 2:** $1 < s < 2$
The overlap is $s-1 < x < 1$:
$$f_S(s) = \int_{s-1}^1 1 \cdot 1 \, dx = 1 - (s-1) = 2 - s$$

**Result:**
$$f_S(s) = \begin{cases}
s, & 0 < s \leq 1 \\
2-s, & 1 < s < 2 \\
0, & \text{otherwise}
\end{cases}$$

This is a **triangular distribution** on $(0,2)$.

### 9.6 Ratios and Products
If $X,Y$ independent positive with densities, product $Z=XY$ uses transformation $(X,Y)\to (Z,Y)$; ratio $R=X/Y$ uses $(R,Y)$. Joint density and Jacobian yield marginal by integration. Special case: ratio of independent standard normals $\Rightarrow t$-like (Cauchy for $Z=X/Y$ when both standard normal).

#### Example 9.6A: Cauchy from Normal Ratio
If $X,Y \overset{iid}{\sim} N(0,1)$ independent, $R = X/Y$ has density $f_R(r)=\frac{1}{\pi(1+r^2)}$ (standard Cauchy) via Jacobian transformation.

### 9.7 Order Statistics
Let $X_{(1)} \le \cdots \le X_{(n)}$ denote ordered i.i.d. sample from CDF $F$ and PDF $f$.
Joint density:
$$f_{X_{(1)},\dots,X_{(n)}}(x_1,\dots,x_n)= n! \prod_{i=1}^n f(x_i), \quad x_1<\cdots<x_n$$

PDF of $k$th order statistic:
$$f_{X_{(k)}}(x)= \frac{n!}{(k-1)!(n-k)!} [F(x)]^{k-1}[1-F(x)]^{n-k} f(x)$$
For Uniform$(0,1)$ this is Beta$(k, n-k+1)$, linking to prior Week 5 content.

#### Example 9.7A: Distribution of the Minimum
$X_{(1)}$ CDF: $F_{X_{(1)}}(x)=1-[1-F(x)]^n$. PDF: $f_{X_{(1)}}(x)= n[1-F(x)]^{n-1} f(x)$. Useful for reliability (time to first failure).

### 9.8 Delta Method Preview
If $\hat{\theta}$ approximately normal: $\hat{\theta} \approx N(\theta, \sigma^2/n)$ then $g(\hat{\theta}) \approx N(g(\theta), (g'(\theta))^2 \sigma^2/n)$. Full formal statement appears in Week 10 (sampling distributions & limit theorems) with second-order extension.

### 9.9 Summary Map
| Task | Tool | Key Formula |
|------|------|-------------|
| One-to-one transform | Inversion | $f_Y(y)=f_X(g^{-1}(y))|d g^{-1}/dy|$ |
| Many-to-one | Root sum | $\sum f_X(x_i(y))|dx_i/dy|$ |
| Multivariate | Jacobian | $f_{U,V}=f_{X,Y}(h(u,v))|J|$ |
| Sum | Convolution | $f_{X+Y}(s)=\int f_X(x)f_Y(s-x)dx$ |
| Order statistic | Combinatorial weighting | $f_{X_{(k)}}$ Beta-kernel |
| Ratio/scale | Transform pairs | Use joint mapping + integrate |

**Quick Jump:** [Week 8](#week-8-midmterm-exam) | [Week 10](#week-10-sampling-distributions-and-limit-theorems) | [Summary](#summary-and-key-points)

---

## Week 10: Sampling Distributions and the Central Limit Theorem

**Reading:** No specific reading assignment listed in syllabus (content typically drawn from Wackerly et al. Chapter 7: Sampling Distributions and the Central Limit Theorem)

**Learning Objectives:**
- Identify sampling distributions
- Understand and apply the central limit theorem
- Calculate approximate probabilities using the normal approximation to the binomial distribution

[Back to Table of Contents](#table-of-contents)

### 10.1 Law of Large Numbers (LLN)
**Weak LLN (WLLN):** For i.i.d. $X_1,\dots,X_n$ with $E[X_i]=\mu$, $\text{Var}(X_i)=\sigma^2<\infty$,
$$\bar{X}_n \xrightarrow{P} \mu$$
**Chebyshev Proof Sketch:** $P(|\bar{X}_n-\mu|>\varepsilon) \le \dfrac{\sigma^2}{n\varepsilon^2} \to 0.$

**Strong LLN (SLLN):** $\bar{X}_n \to \mu$ almost surely (Kolmogorov conditions; for i.i.d. with $E|X|<\infty$ it holds).

### 10.2 Central Limit Theorem (CLT)
If $X_i$ i.i.d. with mean $\mu$ and variance $\sigma^2>0$, then
$$\frac{\bar{X}_n-\mu}{\sigma/\sqrt{n}} \xrightarrow{d} N(0,1)$$
**Lindeberg–Feller (generalized) CLT** handles independent non-identical with Lindeberg condition.

### 10.3 Standardizing and Normal Approximation
For sums $S_n=\sum_{i=1}^n X_i$:
$$\frac{S_n - n\mu}{\sigma\sqrt{n}} \Rightarrow N(0,1)$$
**Berry–Esseen Bound:** Provides rate $|P((S_n-n\mu)/(\sigma\sqrt{n}) \le x)-\Phi(x)| \le C \frac{E|X_1-\mu|^3}{\sigma^3\sqrt{n}}$.

### 10.4 Chi-Square, t, and F Distributions
**Recap:** If $Z_i \sim N(0,1)$ i.i.d., then $\sum Z_i^2 \sim \chi^2(k)$. If $X \sim N(0,1)$ independent of $Y \sim \chi^2(\nu)$ then
$$T = \frac{X}{\sqrt{Y/\nu}} \sim t_\nu$$
If $Y_1/\nu_1 \sim \chi^2(\nu_1)/\nu_1$ and $Y_2/\nu_2 \sim \chi^2(\nu_2)/\nu_2$ independent, then $(Y_1/\nu_1)/(Y_2/\nu_2) \sim F_{\nu_1,\nu_2}$.

### 10.5 Delta Method Basics
If $\sqrt{n}(\hat{\theta}_n-\theta) \Rightarrow N(0,\tau^2)$ and $g$ differentiable with $g'(\theta) \neq 0$, then
$$\sqrt{n}(g(\hat{\theta}_n)-g(\theta)) \Rightarrow N(0, (g'(\theta))^2 \tau^2)$$
**Second-order version:** if $g'(\theta)=0$, use $g''(\theta)$ with variance scaling factor $(g''(\theta)/2)^2 \tau^4$.

### 10.6 Sampling Distribution Examples
1. Sample mean of Poisson($\lambda$) approximately normal for moderate $n$.
2. Binomial proportion $\hat{p}=X/n$ approx $N(p, p(1-p)/n)$.
3. Log-transform: $g(\hat{p})=\log(\hat{p})$ delta method variance $\approx (1/p^2) p(1-p)/n$.

### 10.7 Continuity Corrections and Normal Approximation Tips
**Continuity Correction:** For $X \sim B(n,p)$,
$$P(X \le k) \approx P\left(Z \le \frac{k+0.5-np}{\sqrt{np(1-p)}}\right)$$

**Rule of Thumb:** Normal approx for proportion reliable if $np(1-p) \gtrsim 10$.

#### Example: CLT Sample Mean Approximation
Given i.i.d. $X_i$ with $E[X_i]=10$, $\text{Var}(X_i)=25$, find $P(\bar{X}_{100} > 11)$. Standardize:
$$Z=\frac{\bar{X}-10}{5/\sqrt{100}}=\frac{\bar{X}-10}{0.5}$$

So $P(\bar{X}>11)=P(Z>2)=1-\Phi(2)\approx 0.0228$.

#### Example: Normal Approximation to Binomial
For $X \sim B(200,0.4)$, compute $P(X \le 70)$. Mean $=80$, SD $=\sqrt{200\cdot0.4\cdot0.6}=6.928$.
Continuity correction: $P(X \le 70) \approx P\left(Z \le \frac{70+0.5-80}{6.928}\right)=P(Z \le -1.37)=0.085.$

#### Example: Delta Method Log-Mean
Let $X_i \sim \text{Exp}(\lambda)$; $\hat{\lambda}=1/\bar{X}$. Consider $g(\hat{\lambda})=\log\hat{\lambda}$. Since $\sqrt{n}(\hat{\lambda}-\lambda) \Rightarrow N(0, \lambda^2)$ and $g'(\lambda)=1/\lambda$, variance becomes $\lambda^2(1/\lambda^2)=1$ so $\sqrt{n}(\log\hat{\lambda}-\log\lambda) \Rightarrow N(0,1)$.

#### Example: Chi-Square from Normal Squares
If $Z_1,\dots,Z_5$ i.i.d. $N(0,1)$ then $\sum Z_i^2 \sim \chi^2(5)$. $P\left(\sum Z_i^2 > 11.07\right)=0.05$ (quantile usage).

---

## Week 11: Estimation

**Reading:** Wackerly et al. Sections 8.1-8.9 (47 pp.)

**Learning Objectives:**
- Compare estimators based on bias, variance, and mean squared error
- Calculate and interpret confidence intervals
- Make sample size calculations

[Back to Table of Contents](#table-of-contents)

### 11.1 Point Estimators and Criteria (Sections 8.1-8.2)

**Definition:** A **point estimator** $\hat{\theta}$ is a statistic (function of the sample data) used to estimate an unknown parameter $\theta$.

**Desirable Properties:**
1. **Unbiasedness:** $E[\hat{\theta}] = \theta$
2. **Consistency:** $\hat{\theta}_n \xrightarrow{P} \theta$ as $n \to \infty$
3. **Efficiency:** Minimum variance among unbiased estimators
4. **Low MSE:** Good balance between bias and variance

**Key Concepts:**
- An estimator is a random variable (varies from sample to sample)
- Different estimators can be used for the same parameter
- We want estimators that are "close" to the true parameter value

### 11.2 Bias, Variance, and MSE

**Bias:**
$$\text{Bias}(\hat{\theta}) = E[\hat{\theta}] - \theta$$

- **Unbiased:** $\text{Bias}(\hat{\theta}) = 0$
- **Asymptotically unbiased:** $\lim_{n \to \infty} \text{Bias}(\hat{\theta}_n) = 0$

**Variance:**
$$\text{Var}(\hat{\theta}) = E[(\hat{\theta} - E[\hat{\theta}])^2]$$

Measures the variability of the estimator across different samples.

**Mean Squared Error (MSE):**
$$\text{MSE}(\hat{\theta}) = E[(\hat{\theta} - \theta)^2] = \text{Var}(\hat{\theta}) + [\text{Bias}(\hat{\theta})]^2$$

**Key Insight:** MSE decomposes into variance (precision) and squared bias (accuracy).

**Consistency:**
An estimator $\hat{\theta}_n$ is **consistent** if:
$$\hat{\theta}_n \xrightarrow{P} \theta \quad \text{as } n \to \infty$$

**Sufficient Condition:** If $\lim_{n \to \infty} \text{MSE}(\hat{\theta}_n) = 0$, then $\hat{\theta}_n$ is consistent.

#### Example 11.2A: Bias and MSE of Sample Variance

**Problem:** For $X_1, \ldots, X_n$ i.i.d. with mean $\mu$ and variance $\sigma^2$, compare:
$$S^2 = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar{X})^2 \quad \text{vs.} \quad \tilde{S}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2$$

**Solution:**

**For $S^2$:**
$$E[S^2] = \sigma^2 \quad \Rightarrow \quad \text{Bias}(S^2) = 0$$
This is why we use $n-1$ in the denominator!

**For $\tilde{S}^2$:**
$$E[\tilde{S}^2] = E\left[\frac{n-1}{n} S^2\right] = \frac{n-1}{n}\sigma^2$$
$$\text{Bias}(\tilde{S}^2) = \frac{n-1}{n}\sigma^2 - \sigma^2 = -\frac{\sigma^2}{n}$$

$\tilde{S}^2$ is biased but asymptotically unbiased since $\lim_{n \to \infty} \text{Bias}(\tilde{S}^2) = 0$.

### 11.3 Method of Moments (MoM)

**Principle:** Equate sample moments to population moments and solve for parameters.

**Procedure:**
1. Compute the first $k$ population moments: $\mu_j' = E[X^j]$ as functions of $\theta_1, \ldots, \theta_k$
2. Compute sample moments: $m_j = \frac{1}{n}\sum_{i=1}^n X_i^j$
3. Solve the system: $m_j = \mu_j'(\theta_1, \ldots, \theta_k)$ for $j = 1, \ldots, k$

**Advantages:**
- Simple and intuitive
- Always produces estimates (when moments exist)
- Consistent under mild conditions

**Disadvantages:**
- May not be efficient
- May produce invalid estimates (e.g., negative variance)

#### Example 11.3A: Gamma Distribution MoM

**Problem:** Let $X_1, \ldots, X_n \sim \text{Gamma}(\alpha, \beta)$ with PDF:
$$f(x; \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}, \quad x > 0$$

Find MoM estimators for $\alpha$ and $\beta$.

**Solution:**

**Step 1:** Population moments
$$E[X] = \frac{\alpha}{\beta}, \quad E[X^2] = \frac{\alpha(\alpha+1)}{\beta^2}$$

So:
$$\text{Var}(X) = E[X^2] - (E[X])^2 = \frac{\alpha}{\beta^2}$$

**Step 2:** Sample moments
$$m_1 = \bar{X}, \quad m_2 = \frac{1}{n}\sum_{i=1}^n X_i^2$$

Sample variance: $s^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \bar{X})^2$

**Step 3:** Equate and solve
From $E[X] = \alpha/\beta$ and $\text{Var}(X) = \alpha/\beta^2$:
$$\frac{\text{Var}(X)}{E[X]} = \frac{1}{\beta} \quad \Rightarrow \quad \hat{\beta}_{\text{MoM}} = \frac{\bar{X}}{s^2}$$

$$\frac{(E[X])^2}{\text{Var}(X)} = \alpha \quad \Rightarrow \quad \hat{\alpha}_{\text{MoM}} = \frac{\bar{X}^2}{s^2}$$

### 11.4 Maximum Likelihood Estimation (MLE)

**Principle:** Choose the parameter value that maximizes the probability (likelihood) of observing the given data.

**Likelihood Function:**
$$L(\theta; x_1, \ldots, x_n) = \prod_{i=1}^n f(x_i; \theta)$$

**Log-Likelihood:**
$$\ell(\theta) = \log L(\theta) = \sum_{i=1}^n \log f(x_i; \theta)$$

**MLE:** The value $\hat{\theta}_{\text{MLE}}$ that maximizes $L(\theta)$ or $\ell(\theta)$:
$$\hat{\theta}_{\text{MLE}} = \arg\max_{\theta} \ell(\theta)$$

**Finding MLE:**
1. Take derivative: $\frac{d\ell(\theta)}{d\theta} = 0$ (score equation)
2. Solve for $\theta$
3. Verify it's a maximum (check second derivative $< 0$)

**Properties of MLE:**
- **Invariance:** If $\hat{\theta}$ is MLE of $\theta$, then $g(\hat{\theta})$ is MLE of $g(\theta)$
- **Asymptotically efficient:** Achieves Cramér-Rao lower bound as $n \to \infty$
- **Asymptotically normal:** $\sqrt{n}(\hat{\theta} - \theta) \xrightarrow{d} N(0, 1/I(\theta))$
- **Consistent** under regularity conditions

#### Example 11.4A: Normal Distribution MLE

**Problem:** Let $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ i.i.d. Find MLEs of $\mu$ and $\sigma^2$.

**Solution:**

**Step 1:** Write the likelihood
$$L(\mu, \sigma^2) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)$$

**Step 2:** Log-likelihood
$$\ell(\mu, \sigma^2) = -\frac{n}{2}\log(2\pi) - \frac{n}{2}\log(\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^n (x_i - \mu)^2$$

**Step 3:** Maximize w.r.t. $\mu$
$$\frac{\partial \ell}{\partial \mu} = \frac{1}{\sigma^2}\sum_{i=1}^n (x_i - \mu) = 0$$
$$\Rightarrow \quad \hat{\mu}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n x_i = \bar{x}$$

**Step 4:** Maximize w.r.t. $\sigma^2$
$$\frac{\partial \ell}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}\sum_{i=1}^n (x_i - \mu)^2 = 0$$

Substitute $\mu = \bar{x}$:
$$\hat{\sigma}^2_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2$$

**Note:** $\hat{\sigma}^2_{\text{MLE}}$ is biased! $E[\hat{\sigma}^2_{\text{MLE}}] = \frac{n-1}{n}\sigma^2$

### 11.5 Fisher Information and Cramér-Rao Lower Bound (Preview)

**Fisher Information (single observation):**
$$I(\theta) = E\left[\left(\frac{\partial}{\partial \theta}\log f(X;\theta)\right)^2\right] = -E\left[\frac{\partial^2}{\partial \theta^2}\log f(X;\theta)\right]$$

**For i.i.d. sample:** $I_n(\theta) = n \cdot I(\theta)$

**Cramér-Rao Lower Bound:**
For any unbiased estimator $\hat{\theta}$:
$$\text{Var}(\hat{\theta}) \geq \frac{1}{I_n(\theta)} = \frac{1}{n \cdot I(\theta)}$$

An unbiased estimator that achieves this bound is called **efficient**.

**Regularity Conditions:**
- Support doesn't depend on $\theta$
- Derivatives can be interchanged with integrals
- Fisher information exists and is finite

#### Example 11.5A: Fisher Information for Exponential

**Problem:** For $X \sim \text{Exp}(\lambda)$, find the Fisher information $I(\lambda)$.

**Solution:**

**Step 1:** PDF and log-likelihood
$$f(x; \lambda) = \lambda e^{-\lambda x}, \quad x > 0$$
$$\log f(x; \lambda) = \log \lambda - \lambda x$$

**Step 2:** Score function
$$\frac{\partial}{\partial \lambda} \log f(X; \lambda) = \frac{1}{\lambda} - X$$

**Step 3:** Fisher information
$$I(\lambda) = E\left[\left(\frac{1}{\lambda} - X\right)^2\right] = E\left[\frac{1}{\lambda^2} - \frac{2X}{\lambda} + X^2\right]$$

Since $E[X] = 1/\lambda$ and $E[X^2] = \text{Var}(X) + (E[X])^2 = 1/\lambda^2 + 1/\lambda^2 = 2/\lambda^2$:
$$I(\lambda) = \frac{1}{\lambda^2} - \frac{2}{\lambda} \cdot \frac{1}{\lambda} + \frac{2}{\lambda^2} = \frac{1}{\lambda^2}$$

**Alternative (using second derivative):**
$$\frac{\partial^2}{\partial \lambda^2} \log f(X; \lambda) = -\frac{1}{\lambda^2}$$
$$I(\lambda) = -E\left[-\frac{1}{\lambda^2}\right] = \frac{1}{\lambda^2}$$

### 11.6 Confidence Intervals (CI) Basics

**Definition:** A $(1-\alpha)100\%$ confidence interval for $\theta$ is a random interval $[L, U]$ such that:
$$P(L \leq \theta \leq U) = 1 - \alpha$$

**Interpretation:** If we repeat the sampling process many times, approximately $(1-\alpha)100\%$ of the constructed intervals will contain the true parameter.

**Common Confidence Intervals:**

**1. Mean (σ known, any distribution, large n):**
$$\bar{X} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$$

**2. Mean (σ unknown, Normal population):**
$$\bar{X} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}$$

**3. Proportion (large n):**
$$\hat{p} \pm z_{\alpha/2} \sqrt{\frac{\hat{p}(1-\hat{p})}{n}}$$

**4. Variance (Normal population):**
$$\left[\frac{(n-1)s^2}{\chi^2_{\alpha/2, n-1}}, \frac{(n-1)s^2}{\chi^2_{1-\alpha/2, n-1}}\right]$$

#### Example 11.6A: Confidence Interval for Mean

**Problem:** A sample of $n = 25$ observations from a normal population yields $\bar{x} = 50$ and $s = 8$. Find a 95% confidence interval for $\mu$.

**Solution:**

Since the population is normal and $\sigma$ is unknown, use the $t$-distribution:
$$\bar{x} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}$$

For 95% CI: $\alpha = 0.05$, $\alpha/2 = 0.025$
From $t$-table: $t_{0.025, 24} \approx 2.064$

$$50 \pm 2.064 \cdot \frac{8}{\sqrt{25}} = 50 \pm 2.064 \cdot 1.6 = 50 \pm 3.30$$

**95% CI:** $[46.70, 53.30]$

### 11.7 Sample Size Determination

**For Estimating a Mean:**
To achieve margin of error $E$ with confidence level $(1-\alpha)$:
$$n = \left(\frac{z_{\alpha/2} \sigma}{E}\right)^2$$

**For Estimating a Proportion:**
$$n = \hat{p}(1-\hat{p})\left(\frac{z_{\alpha/2}}{E}\right)^2$$

If no prior estimate of $p$: use $\hat{p} = 0.5$ (most conservative, gives largest $n$).

#### Example 11.7A: Sample Size for Mean

**Problem:** How large a sample is needed to estimate the mean lifetime of batteries with margin of error $E = 2$ hours and 95% confidence, if $\sigma = 10$ hours?

**Solution:**

$$n = \left(\frac{z_{\alpha/2} \sigma}{E}\right)^2 = \left(\frac{1.96 \times 10}{2}\right)^2 = (9.8)^2 = 96.04$$

Round up: $n = 97$ batteries needed.

### 11.8 Sufficiency (Factorization Theorem) Preview

**Intuitive Definition:** A statistic $T(X)$ is **sufficient** for $\theta$ if it contains all the information in the sample about $\theta$. Given $T$, the data provides no additional information about $\theta$.

**Formal Definition:** $T(X)$ is sufficient for $\theta$ if the conditional distribution of $X$ given $T(X) = t$ does not depend on $\theta$.

**Factorization Theorem (Neyman-Fisher):**
$T(X)$ is sufficient for $\theta$ if and only if the likelihood can be factored as:
$$L(\theta; x) = g(T(x), \theta) \cdot h(x)$$
where $g$ depends on $x$ only through $T(x)$, and $h$ doesn't depend on $\theta$.

Full development in Week 12.

### 11.9 Worked Foundational Estimation Examples

#### Example 11.9A: Bernoulli MLE and Bias

**Problem:** Let $X_1, \ldots, X_n \sim \text{Bernoulli}(p)$ i.i.d. Find the MLE of $p$ and check if it's unbiased.

**Solution:**

**Step 1:** Likelihood
$$L(p) = \prod_{i=1}^n p^{x_i}(1-p)^{1-x_i} = p^{\sum x_i}(1-p)^{n - \sum x_i}$$

**Step 2:** Log-likelihood
$$\ell(p) = \left(\sum_{i=1}^n x_i\right) \log p + \left(n - \sum_{i=1}^n x_i\right) \log(1-p)$$

**Step 3:** Maximize
$$\frac{d\ell}{dp} = \frac{\sum x_i}{p} - \frac{n - \sum x_i}{1-p} = 0$$

Solving:
$$\hat{p}_{\text{MLE}} = \frac{1}{n}\sum_{i=1}^n x_i = \bar{X}$$

**Step 4:** Check bias
$$E[\hat{p}] = E[\bar{X}] = E[X_1] = p$$

The MLE is unbiased! ✓

#### Example 11.9B: Poisson Confidence Interval

**Problem:** In a sample of $n = 100$ time intervals, we observe a total of 250 events. Construct a 95% CI for the Poisson rate $\lambda$.

**Solution:**

**Step 1:** MLE of $\lambda$
$$\hat{\lambda} = \bar{X} = \frac{250}{100} = 2.5$$

**Step 2:** Use asymptotic normality
For large $n$: $\hat{\lambda} \approx N\left(\lambda, \frac{\lambda}{n}\right)$

Standard error: $\text{SE}(\hat{\lambda}) = \sqrt{\frac{\hat{\lambda}}{n}} = \sqrt{\frac{2.5}{100}} = 0.158$

**Step 3:** 95% CI
$$2.5 \pm 1.96 \times 0.158 = 2.5 \pm 0.310$$

**95% CI:** $[2.19, 2.81]$

---

## Week 12: Properties of Point Estimation and Methods of Estimation

**Reading:** Wackerly et al. Sections 9.1-9.8 (41 pp.)

**Learning Objectives:**
- Understand and apply principles of relative efficiency, consistency, and sufficiency
- Derive estimators based on the method of moments and the maximum likelihood principle

[Back to Table of Contents](#table-of-contents)

### 12.1 Sufficiency (Formal) (Sections 9.1, 9.4)

**Factorization Theorem (Neyman-Fisher):**
A statistic $T(X_1, \ldots, X_n)$ is **sufficient** for parameter $\theta$ if and only if the joint PDF/PMF can be factored as:
$$f(x_1, \ldots, x_n; \theta) = g(T(x_1, \ldots, x_n), \theta) \cdot h(x_1, \ldots, x_n)$$
where:
- $g$ depends on the data only through $T$ and may depend on $\theta$
- $h$ depends on the data but NOT on $\theta$

**Key Insight:** All information about $\theta$ in the sample is captured by $T$.

**Exponential Family:**
If the distribution belongs to the exponential family:
$$f(x; \theta) = h(x) c(\theta) \exp\left(\sum_{j=1}^k w_j(\theta) t_j(x)\right)$$
then $T(X) = \left(\sum_{i=1}^n t_1(X_i), \ldots, \sum_{i=1}^n t_k(X_i)\right)$ is sufficient for $\theta$.

#### Example 12.1A: Sufficiency for Normal Mean

**Problem:** Let $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ i.i.d. with $\sigma^2$ known. Show that $\bar{X}$ is sufficient for $\mu$.

**Solution:**

**Step 1:** Write the joint PDF
$$f(x_1, \ldots, x_n; \mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)$$

**Step 2:** Simplify the exponent
$$\sum_{i=1}^n (x_i - \mu)^2 = \sum_{i=1}^n x_i^2 - 2\mu\sum_{i=1}^n x_i + n\mu^2$$

**Step 3:** Factor the likelihood
$$f(x_1, \ldots, x_n; \mu) = \left(\frac{1}{(2\pi\sigma^2)^{n/2}}\right) \exp\left(-\frac{\sum x_i^2}{2\sigma^2}\right) \cdot \exp\left(\frac{\mu \sum x_i}{\sigma^2} - \frac{n\mu^2}{2\sigma^2}\right)$$

This can be written as:
$$f = h(x_1, \ldots, x_n) \cdot g(\bar{x}, \mu)$$

where $g$ depends on the data only through $\bar{x} = \frac{1}{n}\sum x_i$ (or equivalently $\sum x_i$).

**Conclusion:** $\bar{X}$ is sufficient for $\mu$. ✓

#### Example 12.1B: Sufficiency for Poisson

**Problem:** Let $X_1, \ldots, X_n \sim \text{Poisson}(\lambda)$ i.i.d. Show that $T = \sum_{i=1}^n X_i$ is sufficient for $\lambda$.

**Solution:**

**Step 1:** Joint PMF
$$P(X_1 = x_1, \ldots, X_n = x_n; \lambda) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} = \frac{\lambda^{\sum x_i} e^{-n\lambda}}{\prod_{i=1}^n x_i!}$$

**Step 2:** Factor
$$= \underbrace{\lambda^{\sum x_i} e^{-n\lambda}}_{g(T, \lambda)} \cdot \underbrace{\frac{1}{\prod_{i=1}^n x_i!}}_{h(x_1, \ldots, x_n)}$$

**Conclusion:** $T = \sum X_i$ is sufficient for $\lambda$. ✓

**Note:** $T \sim \text{Poisson}(n\lambda)$ by the additive property of Poisson.

### 12.2 Completeness and Lehmann-Scheffé

**Completeness:**
A statistic $T$ is **complete** for $\theta$ if:
$$E[g(T)] = 0 \text{ for all } \theta \implies P(g(T) = 0) = 1 \text{ for all } \theta$$

In other words, no non-trivial function of $T$ has zero expectation for all parameter values.

**Lehmann-Scheffé Theorem:**
If $T$ is a **complete sufficient statistic** for $\theta$ and $\phi(T)$ is an unbiased estimator of $\tau(\theta)$, then $\phi(T)$ is the **unique** minimum variance unbiased estimator (UMVUE or UMVU) of $\tau(\theta)$.

**Strategy for finding UMVUE:**
1. Find a complete sufficient statistic $T$
2. Find any unbiased estimator $\hat{\tau}$ of $\tau(\theta)$
3. Compute $\phi(T) = E[\hat{\tau} | T]$ (Rao-Blackwell improvement)
4. $\phi(T)$ is the UMVUE

#### Example 12.2A: UMVUE for Poisson

**Problem:** Let $X_1, \ldots, X_n \sim \text{Poisson}(\lambda)$ i.i.d. Find the UMVUE of $e^{-\lambda}$ (the probability of zero events).

**Solution:**

**Step 1:** Complete sufficient statistic
$T = \sum_{i=1}^n X_i \sim \text{Poisson}(n\lambda)$ is complete and sufficient.

**Step 2:** Find an unbiased estimator
Note that $E[I(X_1 = 0)] = P(X_1 = 0) = e^{-\lambda}$

So $I(X_1 = 0)$ is an unbiased estimator of $e^{-\lambda}$.

**Step 3:** Condition on the sufficient statistic
$$\phi(T) = E[I(X_1 = 0) | T = t]$$

By symmetry and the fact that $T = X_1 + \sum_{i=2}^n X_i$:
$$P(X_1 = 0 | T = t) = P(X_1 = 0, \sum_{i=2}^n X_i = t) / P(T = t)$$

Since $X_1 \sim \text{Poisson}(\lambda)$ and $\sum_{i=2}^n X_i \sim \text{Poisson}((n-1)\lambda)$ are independent:
$$= \frac{e^{-\lambda} \cdot \frac{((n-1)\lambda)^t e^{-(n-1)\lambda}}{t!}}{\frac{(n\lambda)^t e^{-n\lambda}}{t!}} = \left(\frac{n-1}{n}\right)^t$$

**Conclusion:** The UMVUE of $e^{-\lambda}$ is:
$$\hat{\theta}_{\text{UMVUE}} = \left(1 - \frac{1}{n}\right)^T = \left(\frac{n-1}{n}\right)^{\sum X_i}$$

### 12.3 Method of Moments vs MLE

**Comparison:**

| **Property** | **Method of Moments** | **Maximum Likelihood** |
|--------------|----------------------|------------------------|
| **Computation** | Usually simpler | May require numerical methods |
| **Efficiency** | Generally less efficient | Asymptotically efficient |
| **Validity** | May give invalid estimates | Usually respects constraints |
| **Invariance** | Not invariant | Invariant under reparameterization |
| **Bias** | Often biased | Often biased in finite samples |
| **Consistency** | Consistent | Consistent |

**When to use MoM:**
- Quick preliminary estimates
- Complex distributions where MLE is intractable
- Starting values for MLE optimization

**When to use MLE:**
- Need optimal properties
- Inference (standard errors, tests)
- Final analysis

#### Example 12.3A: Uniform(0, θ) Comparison

**Problem:** Let $X_1, \ldots, X_n \sim \text{Uniform}(0, \theta)$ i.i.d. Compare MoM and MLE.

**Solution:**

**Method of Moments:**
$$E[X] = \frac{\theta}{2} = \bar{X} \implies \hat{\theta}_{\text{MoM}} = 2\bar{X}$$

**Maximum Likelihood:**
The likelihood is:
$$L(\theta) = \frac{1}{\theta^n} \cdot I(\theta \geq \max\{x_i\})$$

This is maximized when $\theta$ is as small as possible while still satisfying the constraint:
$$\hat{\theta}_{\text{MLE}} = \max\{X_1, \ldots, X_n\} = X_{(n)}$$

**Comparison:**
- $E[\hat{\theta}_{\text{MLE}}] = \frac{n}{n+1}\theta$ (biased downward)
- $E[\hat{\theta}_{\text{MoM}}] = \theta$ (unbiased)
- $\text{Var}(\hat{\theta}_{\text{MLE}}) = \frac{n\theta^2}{(n+1)^2(n+2)}$
- $\text{Var}(\hat{\theta}_{\text{MoM}}) = \frac{\theta^2}{3n}$

For large $n$, MLE has smaller variance despite being biased!

**Unbiased version of MLE:**
$$\hat{\theta}_{\text{Unbiased}} = \frac{n+1}{n}X_{(n)}$$
has $\text{Var} = \frac{\theta^2}{n(n+2)} < \text{Var}(\hat{\theta}_{\text{MoM}})$

### 12.4 Fisher Information and Cramér-Rao Lower Bound (Detailed)

**Fisher Information (alternative forms):**
$$I(\theta) = E\left[\left(\frac{\partial \log f(X;\theta)}{\partial \theta}\right)^2\right] = -E\left[\frac{\partial^2 \log f(X;\theta)}{\partial \theta^2}\right]$$

**Regularity Conditions:**
1. Parameter space is an open interval
2. Support doesn't depend on $\theta$
3. $f(x; \theta)$ is twice differentiable w.r.t. $\theta$
4. Interchange of differentiation and integration is valid
5. $0 < I(\theta) < \infty$

**Cramér-Rao Lower Bound (CRLB):**
For any estimator $\hat{\theta}$ (not necessarily unbiased):
$$\text{Var}(\hat{\theta}) \geq \frac{[1 + b'(\theta)]^2}{I_n(\theta)}$$
where $b(\theta) = E[\hat{\theta}] - \theta$ is the bias.

For unbiased estimators ($b(\theta) = 0$):
$$\text{Var}(\hat{\theta}) \geq \frac{1}{I_n(\theta)}$$

**Efficiency:**
The **efficiency** of an unbiased estimator is:
$$\text{eff}(\hat{\theta}) = \frac{1/I_n(\theta)}{\text{Var}(\hat{\theta})} \in [0, 1]$$

An estimator with efficiency 1 is called **efficient**.

#### Example 12.4A: Fisher Information for Bernoulli

**Problem:** For $X \sim \text{Bernoulli}(p)$, find $I(p)$ and verify the CRLB for $\hat{p} = \bar{X}$.

**Solution:**

**Step 1:** Log-likelihood
$$\log f(x; p) = x\log p + (1-x)\log(1-p)$$

**Step 2:** First derivative (score)
$$\frac{\partial}{\partial p}\log f(X; p) = \frac{X}{p} - \frac{1-X}{1-p} = \frac{X - p}{p(1-p)}$$

**Step 3:** Fisher information
$$I(p) = E\left[\left(\frac{X - p}{p(1-p)}\right)^2\right] = \frac{E[(X-p)^2]}{p^2(1-p)^2} = \frac{p(1-p)}{p^2(1-p)^2} = \frac{1}{p(1-p)}$$

**Step 4:** CRLB for sample of size $n$
$$\text{Var}(\hat{p}) \geq \frac{1}{nI(p)} = \frac{p(1-p)}{n}$$

**Step 5:** Verify $\hat{p} = \bar{X}$ achieves bound
$$\text{Var}(\bar{X}) = \frac{\text{Var}(X_1)}{n} = \frac{p(1-p)}{n}$$

**Conclusion:** $\bar{X}$ is efficient for $p$! ✓

### 12.5 Asymptotic Normality of MLE

**Theorem:** Under regularity conditions, the MLE $\hat{\theta}_n$ satisfies:
$$\sqrt{n}(\hat{\theta}_n - \theta) \xrightarrow{d} N\left(0, \frac{1}{I(\theta)}\right)$$

Equivalently:
$$\hat{\theta}_n \xrightarrow{d} N\left(\theta, \frac{1}{nI(\theta)}\right)$$

**Implications:**
1. **Asymptotic CI:** $\hat{\theta}_n \pm z_{\alpha/2}/\sqrt{nI(\hat{\theta}_n)}$
2. **Wald test statistic:** $W = n(\hat{\theta}_n - \theta_0)^2 I(\hat{\theta}_n) \xrightarrow{d} \chi^2(1)$ under $H_0: \theta = \theta_0$
3. **Standard error:** $\text{SE}(\hat{\theta}_n) \approx 1/\sqrt{nI(\hat{\theta}_n)}$

#### Example 12.5A: Asymptotic CI for Poisson Rate

**Problem:** For $X_1, \ldots, X_n \sim \text{Poisson}(\lambda)$ with $\bar{X} = 3.2$ and $n = 50$, construct a 95% asymptotic CI for $\lambda$.

**Solution:**

**Step 1:** MLE
$$\hat{\lambda} = \bar{X} = 3.2$$

**Step 2:** Fisher information for Poisson
For $X \sim \text{Poisson}(\lambda)$: $I(\lambda) = 1/\lambda$

**Step 3:** Asymptotic variance
$$\text{Var}(\hat{\lambda}) \approx \frac{1}{nI(\lambda)} = \frac{\lambda}{n}$$

Estimate: $\widehat{\text{Var}}(\hat{\lambda}) = \frac{3.2}{50} = 0.064$

**Step 4:** Standard error
$$\text{SE}(\hat{\lambda}) = \sqrt{0.064} = 0.253$$

**Step 5:** 95% CI
$$3.2 \pm 1.96 \times 0.253 = 3.2 \pm 0.496$$

**95% CI:** $[2.70, 3.70]$

### 12.6 Efficiency and Comparing Estimators

**Relative Efficiency:**
For two unbiased estimators $\hat{\theta}_1$ and $\hat{\theta}_2$:
$$\text{eff}(\hat{\theta}_1, \hat{\theta}_2) = \frac{\text{Var}(\hat{\theta}_2)}{\text{Var}(\hat{\theta}_1)}$$

If $\text{eff} > 1$, then $\hat{\theta}_1$ is more efficient (preferred).

**Asymptotic Relative Efficiency (ARE):**
$$\text{ARE}(\hat{\theta}_1, \hat{\theta}_2) = \lim_{n \to \infty} \frac{\text{Var}(\hat{\theta}_2)}{\text{Var}(\hat{\theta}_1)}$$

**Bias-Variance Tradeoff:**
Sometimes a biased estimator with lower variance can have smaller MSE:
$$\text{MSE}(\hat{\theta}) = \text{Var}(\hat{\theta}) + [\text{Bias}(\hat{\theta})]^2$$

**James-Stein Phenomenon:** For $\theta \in \mathbb{R}^p$ with $p \geq 3$, shrinkage estimators can dominate the MLE in terms of MSE.

### 12.7 Worked Advanced Estimation Examples

#### Example 12.7A: Exponential Family Sufficient Statistic

**Problem:** Show that the exponential distribution with PDF $f(x; \lambda) = \lambda e^{-\lambda x}$ is in the exponential family and find the sufficient statistic.

**Solution:**

**Step 1:** Write in exponential family form
$$f(x; \lambda) = \exp(\log \lambda - \lambda x) = \underbrace{1}_{h(x)} \underbrace{\lambda}_{c(\lambda)} \exp(\underbrace{-\lambda}_{w(\lambda)} \cdot \underbrace{x}_{t(x)})$$

**Step 2:** For i.i.d. sample
$$f(x_1, \ldots, x_n; \lambda) = \lambda^n \exp\left(-\lambda \sum_{i=1}^n x_i\right)$$

**Step 3:** Sufficient statistic
$$T = \sum_{i=1}^n X_i$$

This is also the MLE: $\hat{\lambda} = 1/\bar{X} = n/T$.

#### Example 12.7B: Lehmann-Scheffé for Variance

**Problem:** For $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ i.i.d. with $\mu$ unknown, find the UMVUE of $\sigma^2$.

**Solution:**

**Step 1:** Complete sufficient statistic
$T = (\bar{X}, S^2)$ where $S^2 = \frac{1}{n-1}\sum(X_i - \bar{X})^2$

**Step 2:** Check unbiasedness
$$E[S^2] = \sigma^2$$

**Step 3:** Apply Lehmann-Scheffé
Since $S^2$ is already a function of the complete sufficient statistic and is unbiased, it is the UMVUE of $\sigma^2$.

**Quick Jump:** [Week 11](#week-11-estimation-fundamentals) | [Week 13](#week-13-hypothesis-testing) | [Summary](#summary-and-key-points)

---

## Week 13: Hypothesis Testing

**Reading:** Wackerly et al. Sections 10.1-10.7 (32 pp.)

**Learning Objectives:**
- Understand and apply fundamental principles of statistical tests of hypotheses
- Calculate power of hypothesis tests and make sample size calculations
- Understand and apply the connection between hypothesis testing and interval estimation
- Calculate and properly interpret p-values

[Back to Table of Contents](#table-of-contents)

### 13.1 Framework and Definitions (Sections 10.1-10.2)

**Hypothesis Testing Setup:**
- **Null hypothesis** $H_0$: Statement we assume to be true (status quo)
- **Alternative hypothesis** $H_1$ (or $H_a$): What we want to provide evidence for

**Types of Tests:**
1. **Two-sided:** $H_0: \theta = \theta_0$ vs $H_1: \theta \neq \theta_0$
2. **One-sided (upper tail):** $H_0: \theta \leq \theta_0$ vs $H_1: \theta > \theta_0$
3. **One-sided (lower tail):** $H_0: \theta \geq \theta_0$ vs $H_1: \theta < \theta_0$

**Test Statistic:** $T(X_1, \ldots, X_n)$ - a function of the data used to make a decision

**Rejection Region (Critical Region):** $R$ - the set of values of $T$ for which we reject $H_0$

**Significance Level:** $\alpha = P(\text{reject } H_0 | H_0 \text{ true})$ - typically 0.05, 0.01, or 0.10

### 13.2 Types of Errors

**Error Types:**

|  | **$H_0$ True** | **$H_1$ True** |
|---|---|---|
| **Reject $H_0$** | Type I Error (α) | Correct Decision (Power) |
| **Fail to Reject $H_0$** | Correct Decision | Type II Error (β) |

**Type I Error (False Positive):**
$$\alpha = P(\text{Reject } H_0 | H_0 \text{ true})$$
- Reject a true null hypothesis
- Controlled by the significance level

**Type II Error (False Negative):**
$$\beta = P(\text{Fail to reject } H_0 | H_1 \text{ true})$$
- Fail to reject a false null hypothesis
- Depends on the true parameter value under $H_1$

**Power:**
$$\text{Power} = 1 - \beta = P(\text{Reject } H_0 | H_1 \text{ true})$$
- Probability of correctly rejecting a false null hypothesis
- Increases with:
  - Larger sample size $n$
  - Larger effect size $|\theta_1 - \theta_0|$
  - Smaller variance $\sigma^2$
  - Larger significance level $\alpha$ (but more Type I errors)

### 13.3 p-Values

**Definition:** The **p-value** is the probability, under $H_0$, of observing a test statistic as extreme or more extreme than the observed value.

**For different tests:**
- **Two-sided:** $p = P(|T| \geq |t_{\text{obs}}| | H_0)$
- **Upper tail:** $p = P(T \geq t_{\text{obs}} | H_0)$
- **Lower tail:** $p = P(T \leq t_{\text{obs}} | H_0)$

**Decision Rule:**
- Reject $H_0$ if $p \leq \alpha$
- Fail to reject $H_0$ if $p > \alpha$

**Interpretation:**
- Small p-value → Strong evidence against $H_0$
- $p \leq 0.01$: Very strong evidence
- $0.01 < p \leq 0.05$: Moderate evidence
- $0.05 < p \leq 0.10$: Weak evidence
- $p > 0.10$: Little or no evidence

**Important:** p-value is NOT the probability that $H_0$ is true!

### 13.4 Confidence Intervals Duality

**Theorem:** There is a one-to-one correspondence between hypothesis tests and confidence intervals.

For a two-sided test at significance level $\alpha$:
- A $(1-\alpha)100\%$ confidence interval for $\theta$ contains all values $\theta_0$ that would NOT be rejected by a two-sided test at level $\alpha$

**Practical Implication:**
- If a $(1-\alpha)$ CI for $\theta$ is $[L, U]$ and $\theta_0 \notin [L, U]$, then we reject $H_0: \theta = \theta_0$ at level $\alpha$
- If $\theta_0 \in [L, U]$, then we fail to reject $H_0: \theta = \theta_0$

**Example:** If a 95% CI for $\mu$ is $[48, 56]$:
- We would reject $H_0: \mu = 45$ at $\alpha = 0.05$ (since $45 \notin [48, 56]$)
- We would NOT reject $H_0: \mu = 50$ at $\alpha = 0.05$ (since $50 \in [48, 56]$)

### 13.5 Common Test Statistics

**1. One-Sample Z-Test (Mean, σ known):**
$$Z = \frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} \sim N(0, 1) \text{ under } H_0$$

**Rejection regions:**
- Two-sided: $|Z| > z_{\alpha/2}$
- Upper tail: $Z > z_\alpha$
- Lower tail: $Z < -z_\alpha$

**2. One-Sample t-Test (Mean, σ unknown, Normal population):**
$$t = \frac{\bar{X} - \mu_0}{s/\sqrt{n}} \sim t_{n-1} \text{ under } H_0$$

**Rejection regions:**
- Two-sided: $|t| > t_{\alpha/2, n-1}$
- Upper tail: $t > t_{\alpha, n-1}$
- Lower tail: $t < -t_{\alpha, n-1}$

**3. One-Sample Proportion Test (large n):**
$$Z = \frac{\hat{p} - p_0}{\sqrt{p_0(1-p_0)/n}} \sim N(0, 1) \text{ under } H_0$$

**4. Chi-Square Test for Variance (Normal population):**
$$\chi^2 = \frac{(n-1)s^2}{\sigma_0^2} \sim \chi^2_{n-1} \text{ under } H_0$$

**5. Two-Sample t-Test (Equal variances):**
$$t = \frac{\bar{X}_1 - \bar{X}_2}{s_p\sqrt{1/n_1 + 1/n_2}} \sim t_{n_1+n_2-2} \text{ under } H_0: \mu_1 = \mu_2$$

where $s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}$ (pooled variance)

### 13.6 Power and Sample Size (Means)

**Power Function:**
$$\pi(\theta) = P(\text{Reject } H_0 | \theta)$$

For $H_0: \mu = \mu_0$ vs $H_1: \mu = \mu_1$ (one-sided, upper tail):

**Power:**
$$\text{Power} = \Phi\left(\frac{\mu_1 - \mu_0}{\sigma/\sqrt{n}} - z_\alpha\right)$$

**Sample Size for Desired Power $(1-\beta)$:**

**One-sided test:**
$$n = \left(\frac{(z_\alpha + z_\beta)\sigma}{\mu_1 - \mu_0}\right)^2$$

**Two-sided test:**
$$n = \left(\frac{(z_{\alpha/2} + z_\beta)\sigma}{\mu_1 - \mu_0}\right)^2$$

where:
- $z_\alpha$ is the critical value for significance level $\alpha$
- $z_\beta$ is chosen so that $\Phi(z_\beta) = 1 - \beta$
- $\delta = \mu_1 - \mu_0$ is the **effect size**

#### Example 13.6A: Sample Size Calculation

**Problem:** A researcher wants to detect a mean difference of $\delta = 5$ units with power 0.90 at significance level 0.05 (two-sided). If $\sigma = 10$, how many subjects are needed?

**Solution:**

**Step 1:** Identify parameters
- $\alpha = 0.05$, so $z_{\alpha/2} = 1.96$
- Power $= 0.90$, so $\beta = 0.10$ and $z_\beta = 1.28$
- $\delta = 5$, $\sigma = 10$

**Step 2:** Apply formula
$$n = \left(\frac{(1.96 + 1.28) \times 10}{5}\right)^2 = \left(\frac{32.4}{5}\right)^2 = (6.48)^2 = 41.99$$

**Step 3:** Round up
$$n = 42 \text{ subjects per group}$$

### 13.7 Multiple Testing (Brief)

**Problem:** When performing $m$ tests, the probability of at least one Type I error increases:
$$P(\text{at least one Type I error}) = 1 - (1-\alpha)^m \approx m\alpha$$

**Family-Wise Error Rate (FWER):** Probability of making at least one Type I error among all tests.

**Bonferroni Correction:**
To control FWER at level $\alpha$, test each hypothesis at level $\alpha/m$:
$$\alpha_{\text{individual}} = \frac{\alpha}{m}$$

**Example:** For $m = 10$ tests with FWER $= 0.05$:
- Test each at level $\alpha = 0.05/10 = 0.005$

**Alternatives:**
- **Holm-Bonferroni:** Step-down procedure (less conservative)
- **Benjamini-Hochberg:** Controls False Discovery Rate (FDR) instead of FWER
- **Šidák correction:** $\alpha_{\text{individual}} = 1 - (1-\alpha)^{1/m}$

### 13.8 Worked Examples

#### Example 13.8A: One-Sample t-Test

**Problem:** A manufacturer claims that batteries have a mean lifetime of 500 hours. A random sample of $n = 25$ batteries yields $\bar{x} = 485$ hours and $s = 40$ hours. Test at $\alpha = 0.05$ whether the true mean is less than 500 hours.

**Solution:**

**Step 1:** Set up hypotheses
- $H_0: \mu = 500$ (or $\mu \geq 500$)
- $H_1: \mu < 500$ (one-sided, lower tail)

**Step 2:** Calculate test statistic
$$t = \frac{485 - 500}{40/\sqrt{25}} = \frac{-15}{8} = -1.875$$

**Step 3:** Find critical value
For $\alpha = 0.05$ and $df = 24$: $t_{0.05, 24} = 1.711$

Rejection region: $t < -1.711$

**Step 4:** Make decision
Since $t = -1.875 < -1.711$, we reject $H_0$.

**Step 5:** p-value
$p = P(t_{24} < -1.875) \approx 0.037$ (from t-table or software)

**Conclusion:** There is sufficient evidence at the 5% level to conclude that the mean battery lifetime is less than 500 hours.

**95% CI:**
$$485 \pm 2.064 \times \frac{40}{\sqrt{25}} = 485 \pm 16.51 = [468.49, 501.51]$$

Note: Since we're doing a one-sided test, a one-sided CI would be more appropriate: $(-\infty, 498.69]$

#### Example 13.8B: Two-Sample t-Test

**Problem:** Two teaching methods are compared. Method A: $n_1 = 30$, $\bar{x}_1 = 78$, $s_1 = 12$. Method B: $n_2 = 35$, $\bar{x}_2 = 72$, $s_2 = 10$. Test at $\alpha = 0.05$ whether the methods differ (assume equal variances).

**Solution:**

**Step 1:** Hypotheses
- $H_0: \mu_1 = \mu_2$ (or $\mu_1 - \mu_2 = 0$)
- $H_1: \mu_1 \neq \mu_2$ (two-sided)

**Step 2:** Pooled variance
$$s_p^2 = \frac{(30-1)(12)^2 + (35-1)(10)^2}{30+35-2} = \frac{29 \times 144 + 34 \times 100}{63} = \frac{4176 + 3400}{63} = \frac{7576}{63} = 120.25$$
$$s_p = 10.97$$

**Step 3:** Test statistic
$$t = \frac{78 - 72}{10.97\sqrt{1/30 + 1/35}} = \frac{6}{10.97 \times 0.248} = \frac{6}{2.72} = 2.206$$

**Step 4:** Critical value
$df = 30 + 35 - 2 = 63$

For $\alpha = 0.05$ (two-sided): $t_{0.025, 63} \approx 2.000$

**Step 5:** Decision
Since $|t| = 2.206 > 2.000$, we reject $H_0$.

**Conclusion:** There is sufficient evidence at the 5% level that the two teaching methods produce different mean scores.

**p-value:** $p \approx 0.031$

#### Example 13.8C: Proportion Test

**Problem:** A company claims that 90% of customers are satisfied. In a sample of $n = 200$ customers, 170 report satisfaction. Test at $\alpha = 0.05$ whether the true proportion differs from 0.90.

**Solution:**

**Step 1:** Hypotheses
- $H_0: p = 0.90$
- $H_1: p \neq 0.90$ (two-sided)

**Step 2:** Sample proportion
$$\hat{p} = \frac{170}{200} = 0.85$$

**Step 3:** Test statistic
$$Z = \frac{0.85 - 0.90}{\sqrt{0.90 \times 0.10 / 200}} = \frac{-0.05}{\sqrt{0.00045}} = \frac{-0.05}{0.0212} = -2.358$$

**Step 4:** Critical value
For $\alpha = 0.05$ (two-sided): $z_{0.025} = 1.96$

**Step 5:** Decision
Since $|Z| = 2.358 > 1.96$, we reject $H_0$.

**p-value:** $p = 2 \times P(Z < -2.358) = 2 \times 0.0092 = 0.0184$

**Conclusion:** There is significant evidence that the true satisfaction rate differs from 90%.

**Quick Jump:** [Week 12](#week-12-properties--methods-of-point-estimation) | [Week 14](#week-14-neyman-pearson-and-likelihood-ratio-tests) | [Summary](#summary-and-key-points)

---

## Week 14: Testing for Means and Variances, Likelihood Ratio Tests

**Reading:** Wackerly et al. Sections 10.8-10.11 (29 pp.)

**Learning Objectives:**
- Understand and apply the Neyman-Pearson lemma
- Carry out hypothesis testing for means and variances
- Construct likelihood ratio tests

[Back to Table of Contents](#table-of-contents)

### 14.1 Neyman-Pearson Lemma (Section 10.8)

**Simple vs Simple Hypotheses:**
- $H_0: \theta = \theta_0$ (simple: single value)
- $H_1: \theta = \theta_1$ (simple: single value)

**Neyman-Pearson Lemma:**
The **most powerful** test of size $\alpha$ for testing $H_0: \theta = \theta_0$ vs $H_1: \theta = \theta_1$ rejects $H_0$ when:
$$\frac{L(\theta_0; x)}{L(\theta_1; x)} \leq k$$

where $k$ is chosen so that $P(\text{reject } H_0 | H_0 \text{ true}) = \alpha$.

Equivalently, reject when the **likelihood ratio**:
$$\Lambda(x) = \frac{L(\theta_0; x)}{L(\theta_1; x)} \leq k$$

**Key Insight:** Among all tests with significance level $\alpha$, the likelihood ratio test has the **highest power**.

**Practical Form:** Often easier to work with:
$$\frac{L(\theta_1; x)}{L(\theta_0; x)} \geq k'$$
or equivalently, reject when $L(\theta_1; x) \geq k'' L(\theta_0; x)$.

#### Example 14.1A: Neyman-Pearson for Normal Mean

**Problem:** Let $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ i.i.d. with $\sigma^2$ known. Find the most powerful test of size $\alpha$ for:
- $H_0: \mu = \mu_0$
- $H_1: \mu = \mu_1$ (where $\mu_1 > \mu_0$)

**Solution:**

**Step 1:** Likelihood ratio
$$\Lambda(x) = \frac{L(\mu_0)}{L(\mu_1)} = \frac{\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x_i-\mu_0)^2/(2\sigma^2)}}{\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x_i-\mu_1)^2/(2\sigma^2)}}$$

**Step 2:** Simplify
$$\Lambda(x) = \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n[(x_i-\mu_0)^2 - (x_i-\mu_1)^2]\right)$$

$$= \exp\left(-\frac{1}{2\sigma^2}\sum_{i=1}^n[2x_i(\mu_1-\mu_0) - \mu_1^2 + \mu_0^2]\right)$$

$$= \exp\left(-\frac{n(\mu_1-\mu_0)}{\sigma^2}\left[\bar{x} - \frac{\mu_1+\mu_0}{2}\right]\right)$$

**Step 3:** Rejection region
Reject $H_0$ when $\Lambda(x) \leq k$.

Since $\mu_1 > \mu_0$, this is equivalent to:
$$\bar{x} \geq c$$

for some constant $c$ determined by $\alpha$.

**Step 4:** Find $c$
Under $H_0$: $\bar{X} \sim N(\mu_0, \sigma^2/n)$

$$P(\bar{X} \geq c | H_0) = \alpha$$
$$P\left(\frac{\bar{X} - \mu_0}{\sigma/\sqrt{n}} \geq \frac{c - \mu_0}{\sigma/\sqrt{n}}\right) = \alpha$$
$$\frac{c - \mu_0}{\sigma/\sqrt{n}} = z_\alpha$$
$$c = \mu_0 + z_\alpha \frac{\sigma}{\sqrt{n}}$$

**Conclusion:** The most powerful test rejects when:
$$\bar{X} \geq \mu_0 + z_\alpha \frac{\sigma}{\sqrt{n}}$$

This is exactly the standard one-sided Z-test!

### 14.2 General Likelihood Ratio Test (LRT)

**Composite Hypotheses:**
- $H_0: \theta \in \Theta_0$ (composite: multiple values)
- $H_1: \theta \in \Theta_1$ (composite: multiple values)
where $\Theta_0 \cup \Theta_1 = \Theta$ and $\Theta_0 \cap \Theta_1 = \emptyset$.

**Likelihood Ratio Statistic:**
$$\lambda(x) = \frac{\sup_{\theta \in \Theta_0} L(\theta; x)}{\sup_{\theta \in \Theta} L(\theta; x)} = \frac{L(\hat{\theta}_0; x)}{L(\hat{\theta}; x)}$$

where:
- $\hat{\theta}_0$ = MLE under $H_0$ (restricted)
- $\hat{\theta}$ = MLE under full parameter space (unrestricted)

**Properties:**
- $0 \leq \lambda(x) \leq 1$
- Smaller $\lambda$ → stronger evidence against $H_0$
- Reject $H_0$ when $\lambda(x) \leq k$ for some $k$

**Wilks' Theorem (Asymptotic Distribution):**
Under $H_0$ and regularity conditions:
$$-2\log \lambda(X) \xrightarrow{d} \chi^2_{\nu}$$
where $\nu = \dim(\Theta) - \dim(\Theta_0)$ (difference in number of free parameters).

**Rejection Region:**
Reject $H_0$ if $-2\log \lambda(x) > \chi^2_{\alpha, \nu}$

#### Example 14.2A: LRT for Normal Variance

**Problem:** Let $X_1, \ldots, X_n \sim N(\mu, \sigma^2)$ i.i.d. with $\mu$ known. Test:
- $H_0: \sigma^2 = \sigma_0^2$
- $H_1: \sigma^2 \neq \sigma_0^2$

**Solution:**

**Step 1:** MLEs
Under $H_0$: $\hat{\sigma}^2_0 = \sigma_0^2$ (given)

Under $\Theta$ (unrestricted): $\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n (X_i - \mu)^2$

**Step 2:** Likelihoods
$$L(\sigma_0^2) = (2\pi\sigma_0^2)^{-n/2} \exp\left(-\frac{1}{2\sigma_0^2}\sum(X_i-\mu)^2\right)$$

$$L(\hat{\sigma}^2) = (2\pi\hat{\sigma}^2)^{-n/2} \exp\left(-\frac{n}{2}\right)$$

**Step 3:** Likelihood ratio
$$\lambda = \left(\frac{\sigma_0^2}{\hat{\sigma}^2}\right)^{n/2} \exp\left(\frac{n}{2}\left(1 - \frac{\sum(X_i-\mu)^2}{n\sigma_0^2}\right)\right)$$

**Step 4:** Simplify
Let $S^2 = \sum_{i=1}^n (X_i - \mu)^2$. Then:
$$-2\log\lambda = n\log\left(\frac{S^2}{n\sigma_0^2}\right) - n + \frac{S^2}{\sigma_0^2}$$

Under $H_0$: $\frac{S^2}{\sigma_0^2} \sim \chi^2_n$

**Step 5:** Test statistic
Equivalently, reject when:
$$\frac{S^2}{\sigma_0^2} < c_1 \text{ or } \frac{S^2}{\sigma_0^2} > c_2$$

where $c_1$ and $c_2$ are chosen from the $\chi^2_n$ distribution.

#### Example 14.2B: LRT for Binomial Proportion

**Problem:** Let $X \sim \text{Binomial}(n, p)$. Test $H_0: p = p_0$ vs $H_1: p \neq p_0$ using LRT.

**Solution:**

**Step 1:** MLEs
Under $H_0$: $\hat{p}_0 = p_0$

Under $\Theta$: $\hat{p} = X/n$

**Step 2:** Likelihoods
$$L(p_0) = \binom{n}{X} p_0^X (1-p_0)^{n-X}$$
$$L(\hat{p}) = \binom{n}{X} \hat{p}^X (1-\hat{p})^{n-X}$$

**Step 3:** LRT statistic
$$\lambda = \left(\frac{p_0}{\hat{p}}\right)^X \left(\frac{1-p_0}{1-\hat{p}}\right)^{n-X}$$

$$-2\log\lambda = 2\left[X\log\left(\frac{\hat{p}}{p_0}\right) + (n-X)\log\left(\frac{1-\hat{p}}{1-p_0}\right)\right]$$

**Step 4:** Asymptotic distribution
Under $H_0$, $-2\log\lambda \xrightarrow{d} \chi^2_1$ (since $\nu = 1 - 0 = 1$)

Reject if $-2\log\lambda > \chi^2_{\alpha, 1}$

**Numerical Example:**
If $n = 100$, $X = 45$, $p_0 = 0.5$:
- $\hat{p} = 0.45$
- $-2\log\lambda = 2[45\log(0.9) + 55\log(1.111)] = 2[-4.74 + 5.79] = 2.10$
- $\chi^2_{0.05, 1} = 3.84$
- Since $2.10 < 3.84$, fail to reject $H_0$

### 14.3 Wald, Score, and LRT Triad

Three asymptotically equivalent tests for $H_0: \theta = \theta_0$ vs $H_1: \theta \neq \theta_0$:

**1. Wald Test:**
Based on MLE $\hat{\theta}$ and its asymptotic normality:
$$W = n(\hat{\theta} - \theta_0)^2 I(\hat{\theta}) \xrightarrow{d} \chi^2_1$$

Reject if $W > \chi^2_{\alpha, 1}$

**Advantage:** Only requires MLE under unrestricted model.

**2. Score Test (Rao's):**
Based on the score function evaluated at $\theta_0$:
$$S(\theta_0) = \frac{\partial \log L(\theta)}{\partial \theta}\Big|_{\theta=\theta_0}$$
$$\text{Score} = \frac{[S(\theta_0)]^2}{nI(\theta_0)} \xrightarrow{d} \chi^2_1$$

Reject if Score $> \chi^2_{\alpha, 1}$

**Advantage:** Only requires estimation under $H_0$.

**3. Likelihood Ratio Test:**
$$\text{LRT} = -2\log\lambda = 2[\log L(\hat{\theta}) - \log L(\theta_0)] \xrightarrow{d} \chi^2_1$$

**Relationship:**
Under regularity conditions and for large $n$:
$$\text{Score} \leq \text{LRT} \leq \text{Wald}$$

All three tests are asymptotically equivalent (same power) but may differ in finite samples.

#### Example 14.3A: Comparing Three Tests for Poisson

**Problem:** For $X_1, \ldots, X_n \sim \text{Poisson}(\lambda)$ with $\sum X_i = 150$, $n = 50$, test $H_0: \lambda = 2.5$ vs $H_1: \lambda \neq 2.5$ at $\alpha = 0.05$ using all three methods.

**Solution:**

**Common setup:**
- $\hat{\lambda} = \bar{X} = 150/50 = 3$
- $I(\lambda) = 1/\lambda$ (Fisher information for Poisson)

**1. Wald Test:**
$$W = n(\hat{\lambda} - \lambda_0)^2 I(\hat{\lambda}) = 50(3 - 2.5)^2 \cdot \frac{1}{3} = 50 \times 0.25 \times 0.333 = 4.17$$

**2. Score Test:**
$$S(\lambda_0) = \sum_{i=1}^n \left(\frac{X_i}{\lambda_0} - 1\right) = \frac{\sum X_i}{\lambda_0} - n = \frac{150}{2.5} - 50 = 10$$
$$\text{Score} = \frac{S^2}{nI(\lambda_0)} = \frac{100}{50 \times (1/2.5)} = \frac{100}{20} = 5.00$$

**3. LRT:**
$$-2\log\lambda = 2[n\hat{\lambda}\log(\hat{\lambda}/\lambda_0) - n(\hat{\lambda} - \lambda_0)]$$
$$= 2[50 \times 3 \times \log(1.2) - 50 \times 0.5] = 2[150 \times 0.182 - 25] = 4.60$$

**Decision:**
Critical value: $\chi^2_{0.05, 1} = 3.84$

All three statistics exceed 3.84, so we reject $H_0$ using all three methods.

Note: Score (5.00) $>$ LRT (4.60) $>$ Wald (4.17), consistent with the general ordering.

### 14.4 Worked Examples

#### Example 14.4A: Two-Sample Variance Test

**Problem:** Test whether two normal populations have equal variances.
- Sample 1: $n_1 = 15$, $s_1^2 = 20$
- Sample 2: $n_2 = 12$, $s_2^2 = 10$

Test $H_0: \sigma_1^2 = \sigma_2^2$ vs $H_1: \sigma_1^2 \neq \sigma_2^2$ at $\alpha = 0.05$.

**Solution:**

**Test Statistic (F-test):**
$$F = \frac{s_1^2}{s_2^2} = \frac{20}{10} = 2.0$$

Under $H_0$: $F \sim F_{14, 11}$ (df = $n_1 - 1$, $n_2 - 1$)

**Critical values:**
- Upper: $F_{0.025, 14, 11} \approx 3.43$
- Lower: $F_{0.975, 14, 11} = 1/F_{0.025, 11, 14} \approx 1/3.09 \approx 0.32$

**Decision:**
Since $0.32 < 2.0 < 3.43$, we fail to reject $H_0$.

**Conclusion:** No significant evidence of unequal variances.

#### Example 14.4B: Goodness-of-Fit Test

**Problem:** A die is rolled 120 times with outcomes:

| Face | 1 | 2 | 3 | 4 | 5 | 6 |
|------|---|---|---|---|---|---|
| Observed | 25 | 18 | 20 | 19 | 22 | 16 |

Test whether the die is fair at $\alpha = 0.05$.

**Solution:**

**Hypotheses:**
- $H_0$: Die is fair ($p_i = 1/6$ for all $i$)
- $H_1$: Die is not fair

**Expected frequencies:** $E_i = 120 \times (1/6) = 20$ for all $i$

**Chi-square test statistic:**
$$\chi^2 = \sum_{i=1}^6 \frac{(O_i - E_i)^2}{E_i}$$
$$= \frac{(25-20)^2}{20} + \frac{(18-20)^2}{20} + \frac{(20-20)^2}{20} + \frac{(19-20)^2}{20} + \frac{(22-20)^2}{20} + \frac{(16-20)^2}{20}$$
$$= \frac{25 + 4 + 0 + 1 + 4 + 16}{20} = \frac{50}{20} = 2.5$$

**Critical value:** $\chi^2_{0.05, 5} = 11.07$ (df = 6 - 1 = 5)

**Decision:** Since $2.5 < 11.07$, fail to reject $H_0$.

**Conclusion:** No significant evidence that the die is unfair.

### 14.5 Power and Sample Size for LRT

For simple vs simple hypotheses ($H_0: \theta = \theta_0$ vs $H_1: \theta = \theta_1$), the power can be approximated using the **non-central chi-square distribution**.

**Non-centrality parameter:**
$$\delta = n \cdot D(\theta_1 || \theta_0)$$

where $D$ is the Kullback-Leibler divergence:
$$D(\theta_1 || \theta_0) = E_{\theta_1}\left[\log\frac{f(X; \theta_1)}{f(X; \theta_0)}\right]$$

Under $H_1$: $-2\log\lambda \sim \chi^2_\nu(\delta)$ (non-central chi-square)

### 14.6 Practical Guidance

**When to use each test:**

1. **Use Wald test when:**
   - MLE is easy to compute
   - Want to construct confidence regions simultaneously
   - Standard errors are readily available

2. **Use Score test when:**
   - MLE under alternative is difficult
   - Testing many constraints simultaneously
   - Restricted model is simpler

3. **Use LRT when:**
   - Testing nested models
   - Want optimal power properties
   - Both restricted and unrestricted MLEs are available

**Important Checks:**
- Verify independence of observations
- Check sample size adequacy for asymptotics
- Examine model identifiability
- Consider robustness to assumptions
- Use exact tests when possible for small samples

**Common Pitfalls:**
- Using LRT for non-nested models
- Ignoring finite-sample corrections
- Misspecifying degrees of freedom
- Forgetting to check regularity conditions

**Quick Jump:** [Week 13](#week-13-hypothesis-testing) | [Summary](#summary-and-key-points)

---

## Summary and Key Points

[Back to Table of Contents](#table-of-contents)

### Core Conceptual Framework
1. **Probability Space Triple:** $(S, \mathcal{F}, P)$
2. **Probability Axioms:** Non-negativity, normalization, countable additivity
3. **Conditional Probability & Independence:** Foundation for Bayes' theorem
4. **Random Variables:** Bridge between probability spaces and real numbers
5. **Distribution Theory:** Unified framework for discrete and continuous distributions
6. **Moment Theory:** Numerical characterization of distributions

### Essential Formula Reference

**Probability Foundations:**
- **Bayes' Theorem:** $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
- **Law of Total Probability:** $P(A) = \sum_i P(A|B_i)P(B_i)$

**Discrete Distributions:**
- **Binomial Distribution:** $P(X=k) = \binom{n}{k}p^k(1-p)^{n-k}$
- **Poisson Distribution:** $P(X=k) = \frac{\lambda^k e^{-\lambda}}{k!}$
- **Geometric Distribution:** $P(X=k) = (1-p)^{k-1}p$

**Continuous Distributions:**
- **Normal Distribution:** $f(x) = \frac{1}{\sigma\sqrt{2\pi}} \exp\left[-\frac{(x-\mu)^2}{2\sigma^2}\right]$
- **Gamma Distribution:** $f(x) = \frac{1}{\beta^\alpha \Gamma(\alpha)} x^{\alpha-1} e^{-x/\beta}$ (scale $\beta$)
- **Beta Distribution:** $f(x) = \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1} (1-x)^{\beta-1}$
- **Chi-Square Distribution:** $f(x) = \frac{1}{2^{k/2}\Gamma(k/2)} x^{k/2-1} e^{-x/2}$ (i.e., Gamma$(k/2, 2)$)

**Moment Generating Functions:**
- **Normal MGF:** $M_X(t) = \exp(\mu t + \frac{\sigma^2t^2}{2})$
- **Gamma MGF:** $M_X(t) = (1 - \beta t)^{-\alpha}$ (scale $\beta$)
- **Chi-Square MGF:** $M_X(t) = (1 - 2t)^{-k/2}$

**Inequalities and Theorems:**
- **Chebyshev's Theorem:** $P(|X - \mu| \geq k\sigma) \leq \frac{1}{k^2}$
- **Markov's Inequality:** $P(X \geq a) \leq \frac{E[X]}{a}$

**Joint Distributions:**
- **Joint PDF Relationship:** $f_{X,Y}(x,y) = \frac{\partial^2 F_{X,Y}(x,y)}{\partial x \partial y}$
- **Marginal PDF:** $f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x,y) \, dy$
- **Conditional PDF:** $f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)}$
- **Law of Total Expectation:** $E[Y] = E[E[Y|X]]$
- **Law of Total Variance:** $\text{Var}(Y) = E[\text{Var}(Y|X)] + \text{Var}(E[Y|X])$

### Study Recommendations
1. **Master Fundamental Concepts:** Build strong foundation in probability axioms and basic calculations
2. **Distribution Mastery:** Understand when and how to apply discrete and continuous distributions
3. **MGF Techniques:** Practice deriving and using moment generating functions for various distributions
4. **Inequality Applications:** Learn to apply Chebyshev's and Markov's inequalities for tail bounds
5. **Joint Distribution Analysis:** Master techniques for working with multiple random variables
6. **Transformation Methods:** Practice Jacobian calculations and bivariate transformations
7. **Real-World Applications:** Connect theoretical concepts to practical statistical problems
8. **Problem Decomposition:** Learn to break complex probability problems into manageable steps

### Weekly Learning Progression
- **Weeks 1-2:** Probability foundations and fundamental laws
- **Weeks 3-4:** Single random variable theory (discrete and continuous)
- **Weeks 5-6:** Advanced distributions and multivariate analysis

---

*This summary now covers material through Weeks 1–14 of Columbia University's P8107 course syllabus: probability foundations; discrete & continuous distributions; inequalities; multivariate & transformations; sampling distributions & limit theorems; estimation fundamentals and advanced properties (sufficiency, completeness, efficiency); and hypothesis testing including Neyman–Pearson and likelihood ratio frameworks—aligned with Wackerly–Mendenhall–Scheaffer and modern mathematical statistics standards.*

---

## Navigation Links

- [Back to Top](#p8107-introduction-to-mathematical-statistics)
- [Table of Contents](#table-of-contents)
- [Quick Navigation](#quick-navigation)

**Good luck with your studies!**
